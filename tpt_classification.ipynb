{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "\n",
    "import time\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "import torchvision.models as models\n",
    "\n",
    "import clip\n",
    "from clip.custom_clip import get_coop\n",
    "from clip.cocoop import get_cocoop\n",
    "from data.imagnet_prompts import imagenet_classes\n",
    "from data.datautils import AugMixAugmenter, build_dataset\n",
    "from utils.tools import Summary, AverageMeter, ProgressMeter, load_model_weight, set_random_seed, create_logger\n",
    "from data.cls_to_names import *\n",
    "from data.fewshot_datasets import fewshot_datasets\n",
    "from data.imagenet_variants import thousand_k_to_200, imagenet_a_mask, imagenet_r_mask, imagenet_v_mask\n",
    "from clip_retrieval.clip_client import ClipClient, Modality\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "client = ClipClient(\n",
    "    url=\"http://127.0.0.1:1234/knn-service\",\n",
    "    indice_name='laion_400m',\n",
    "    modality=Modality.IMAGE,\n",
    "    num_images=10,\n",
    "    deduplicate=False,\n",
    ")\n",
    "client_backup = ClipClient(\n",
    "    url=\"http://127.0.0.1:1234/knn-service\",\n",
    "    indice_name='laion_400m',\n",
    "    modality=Modality.IMAGE,\n",
    "    num_images=200,\n",
    "    deduplicate=False,\n",
    ")\n",
    "\n",
    "client_backup2 = ClipClient(\n",
    "    url=\"http://127.0.0.1:1234/knn-service\",\n",
    "    indice_name='laion_400m',\n",
    "    modality=Modality.IMAGE,\n",
    "    num_images=1000,\n",
    "    deduplicate=False,\n",
    ")\n",
    "\n",
    "## Class to names mapping\n",
    "fewshot_datasets = ['DTD', 'Flower102', 'Food101', 'Cars', 'SUN397', \n",
    "                    'Aircraft', 'Pets', 'Caltech101', 'UCF101', 'eurosat']\n",
    "test_sets = 'Cars'\n",
    "if test_sets == 'Caltech101':\n",
    "    cls2names = ['face', 'leopard', 'motorbike', 'accordion', 'airplane', 'anchor', 'ant', 'barrel', 'bass', 'beaver', 'binocular', 'bonsai', 'brain', 'brontosaurus', 'buddha', 'butterfly', 'camera', 'cannon', 'car_side', 'ceiling_fan', 'cellphone', 'chair', 'chandelier', 'cougar_body', 'cougar_face', 'crab', 'crayfish', 'crocodile', 'crocodile_head', 'cup', 'dalmatian', 'dollar_bill', 'dolphin', 'dragonfly', 'electric_guitar', 'elephant', 'emu', 'euphonium', 'ewer', 'ferry', 'flamingo', 'flamingo_head', 'garfield', 'gerenuk', 'gramophone', 'grand_piano', 'hawksbill', 'headphone', 'hedgehog', 'helicopter', 'ibis', 'inline_skate', 'joshua_tree', 'kangaroo', 'ketch', 'lamp', 'laptop', 'llama', 'lobster', 'lotus', 'mandolin', 'mayfly', 'menorah', 'metronome', 'minaret', 'nautilus', 'octopus', 'okapi', 'pagoda', 'panda', 'pigeon', 'pizza', 'platypus', 'pyramid', 'revolver', 'rhino', 'rooster', 'saxophone', 'schooner', 'scissors', 'scorpion', 'sea_horse', 'snoopy', 'soccer_ball', 'stapler', 'starfish', 'stegosaurus', 'stop_sign', 'strawberry', 'sunflower', 'tick', 'trilobite', 'umbrella', 'watch', 'water_lilly', 'wheelchair', 'wild_cat', 'windsor_chair', 'wrench', 'yin_yang']\n",
    "elif test_sets == 'DTD':\n",
    "    cls2names = dtd_classes\n",
    "elif test_sets =='Cars':\n",
    "    cls2names = cars_classes\n",
    "    # print(cls2names[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_confident_samples(logits, top):\n",
    "    batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "    idx = torch.argsort(batch_entropy, descending=False)[:int(batch_entropy.size()[0] * top)]\n",
    "    return logits[idx], idx\n",
    "\n",
    "def avg_entropy(outputs):\n",
    "    # epsilon = 1e-10\n",
    "    assert len(outputs) > 0\n",
    "    assert torch.any(torch.isnan(outputs)) == False\n",
    "    logits = outputs - outputs.logsumexp(dim=-1, keepdim=True) # logits = outputs.log_softmax(dim=1) [N, 1000]\n",
    "    assert torch.any(torch.isnan(logits)) == False\n",
    "    avg_logits = logits.logsumexp(dim=0) - np.log(logits.shape[0]) # avg_logits = logits.mean(0) [1, 1000]\n",
    "    # print(avg_logits)\n",
    "    if torch.any(torch.isnan(avg_logits)):\n",
    "        print(\"average logits \", outputs.log_softmax(dim=1).mean(0))\n",
    "    assert torch.any(torch.isnan(avg_logits)) == False\n",
    "    \n",
    "    min_real = torch.finfo(avg_logits.dtype).min\n",
    "    avg_logits = torch.clamp(avg_logits, min=min_real)\n",
    "    assert torch.any(torch.isnan(avg_logits)) == False\n",
    "    return -((avg_logits) * (torch.exp(avg_logits))).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy(output, target, topk=(1,), caption=None, logger=None):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "        if output.shape[0] == 1:#only image prediction\n",
    "            logit_k, pred = output.topk(maxk, 1, True, True)\n",
    "            pred = pred.t()\n",
    "            correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        else: # evaluate captions\n",
    "            bag = []\n",
    "            # # length = max(5, output.shape[0]-1)\n",
    "            # cap_pred = output[1:]\n",
    "            # # cap_pred = torch.mean(cap_pred, 0,  keepdim=True)\n",
    "            # _, pred = cap_pred.topk(maxk, 1, True, True) #5, 1 #candidate labels\n",
    "            # pred = pred.reshape(maxk, 1)\n",
    "            pred = torch.mean(output, 0, keepdim=True)\n",
    "            _, pred = pred.topk(maxk, 1, True, True)\n",
    "            pred = pred.reshape(maxk, 1)\n",
    "\n",
    "            print(\"caption prediction\" , pred)\n",
    "            print(\"label \", target.view(1, -1).expand_as(pred))\n",
    "            correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            if k == 1 and correct_k.item() == 0:\n",
    "                print(\"-------wrong prediction-------\")\n",
    "                # logger.info(\"wrong prediction , logit: \", output)\n",
    "                pred = pred.squeeze().tolist()\n",
    "                pred = [cls2names[lb] for lb in pred]\n",
    "                print(\"target: [{}]\".format( cls2names[target.squeeze().item()]))\n",
    "                print(\"predicted category & logit: {}\".format(list(zip(pred, logit_k.squeeze().tolist()))))\n",
    "                # print(\"logit \", logit_k)\n",
    "                if logger: logger.info(\"wrong prediction, target {} & predicted value {}\".format(target, pred))\n",
    "                print(\"-------------------------------\")\n",
    "            elif k==1 and correct_k.item() == 1:\n",
    "                print(\"-------correct prediction-------\")\n",
    "                # logger.info(\"wrong prediction , logit: \", output)\n",
    "                pred = pred.squeeze().tolist()\n",
    "                pred = [cls2names[lb] for lb in pred]\n",
    "                print(\"target: [{}]\".format( cls2names[target.squeeze().item()]))\n",
    "                print(\"predicted category & logit: {}\".format(list(zip(pred, logit_k.squeeze().tolist()))))\n",
    "                print(\"-------------------------------\")\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "tta_steps = 1\n",
    "which_loss = \"cosine\"\n",
    "gpu = 3\n",
    "print_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_time_tuning(model, inputs, optimizer, scaler, imagepath = None):\n",
    "    # Entropy + Triplet loss function * 0.5\n",
    "    # Triplet loss function, anchor = retrieved vocab, positive = top5, negative = bottom5\n",
    "    for j in range(tta_steps):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output_img, text_features = model(inputs) # bs, n_cls, (1, 1000), logit/ n_cls, 512\n",
    "            logit_scale = model.logit_scale.exp()\n",
    "            loss = 0\n",
    "            ent = avg_entropy(output_img)\n",
    "            if which_loss in [\"both\", \"cosine\"]:\n",
    "                print(\"Pretrained model Entropy: \", ent.item())\n",
    "                retrieve_K = 4 #TODO: 1,2,4,8\n",
    "                # retrieved_txt_prj, _ = model.nn_project(image=inputs, print_hits=False, topk=retrieve_K) # K, 512, retrieved text projection\n",
    "                # assert len(retrieved_txt_prj) == retrieve_K, len(retrieved_txt_prj)\n",
    "                try:\n",
    "                    query_res = client.query(image=imagepath)[:retrieve_K]\n",
    "                    retrieved_txt= [D['caption'] for D in query_res]\n",
    "                    retrieved_url = [D['url'] for D in query_res]\n",
    "                    retrieved_score = [D['similarity'] for D in query_res]\n",
    "                except:\n",
    "                    # print(client.query(image=imagepath))\n",
    "                    query_res = client_backup.query(image=imagepath)[:retrieve_K]\n",
    "                    retrieved_txt= [D['caption'] for D in query_res]\n",
    "                    retrieved_url = [D['url'] for D in query_res]\n",
    "                    retrieved_score = [D['similarity'] for D in query_res]\n",
    "                if len(retrieved_txt) == retrieve_K:\n",
    "                    caption_feat = model.forward_caption(retrieved_txt) #normalized, \n",
    "                    # output_merged = model.caption_ensemble(retrieved_txt, retrieved_score)\n",
    "                else:\n",
    "                    query_res = client_backup2.query(image=imagepath)[:retrieve_K]\n",
    "                    retrieved_txt= [D['caption'] for D in query_res]\n",
    "                    retrieved_url = [D['url'] for D in query_res]\n",
    "                    retrieved_score = [D['similarity'] for D in query_res]\n",
    "                    caption_feat = model.forward_caption(retrieved_txt)\n",
    "                \n",
    "                for i, c_i in enumerate(caption_feat):\n",
    "                    print(\"Caption: \", retrieved_txt[i])\n",
    "                    print(\"paired Image: \", retrieved_url[i])\n",
    "                    c_i = c_i.reshape(1, -1)\n",
    "                    ent = avg_entropy(logit_scale * c_i @ text_features.t())\n",
    "                    print(\"caption entropy \", ent)\n",
    "                \n",
    "                # merged\n",
    "                weighted =[]\n",
    "                assert len(retrieved_score) == len(caption_feat), (len(retrieved_score), len(caption_feat))\n",
    "                # print()\n",
    "                for score, caption in zip(retrieved_score, caption_feat):\n",
    "                    weighted.append(score/sum(retrieved_score) * caption)\n",
    "                weighted = torch.mean(torch.stack(weighted), dim=0).reshape(1, -1)\n",
    "                # assert weighted.shape == (1, img_feat.shape[-1]), weighted.shape\n",
    "                criterion = torch.nn.CosineEmbeddingLoss()\n",
    "                \n",
    "                K=4\n",
    "                top, top_ind = output_img.topk(2*K, 1, True, True)\n",
    "                topK_ind = top_ind.squeeze()[:K]\n",
    "                botK_ind = top_ind.squeeze()[K:]\n",
    "                label = torch.tensor([1 for _ in range(K)] + [-1 for _ in range(K)]).cuda(gpu)\n",
    "                pred = [cls2names[lb] for lb in topK_ind]\n",
    "                # print(text_features[top_ind].shape, weighted.shape, label.shape)\n",
    "                x1 = text_features[top_ind].squeeze()\n",
    "                loss += criterion(x1, weighted.expand_as(x1), label)\n",
    "                \n",
    "                print(\"------before tuning --------\")\n",
    "                \n",
    "                print(\"Prediced category & logit {}\".format(list(zip(pred, top.squeeze()[:K]))))\n",
    "                \n",
    "                ## caption prediction\n",
    "                caption_merged_logit = logit_scale * weighted @ text_features.t()\n",
    "                top, top_ind = caption_merged_logit.topk(2*K, 1, True, True)\n",
    "                topK_ind = top_ind.squeeze()[:K]\n",
    "                botK_ind = top_ind.squeeze()[K:]\n",
    "                pred = [cls2names[lb] for lb in topK_ind]\n",
    "                print(\"Predicted category, logit of caption {}\".format(list(zip(pred, top.squeeze()[:K]))))\n",
    "                \n",
    "                optimizer.zero_grad() \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                # K = 5\n",
    "                # top, top_ind = output_img.topk(2*K, 1, True, True)\n",
    "                # # # print(top_ind.shape)\n",
    "                # top5_ind = top_ind.squeeze()[:K]\n",
    "                # bot5_ind = top_ind.squeeze()[K:]\n",
    "                # assert len(top5_ind) == K and len(bot5_ind) == K, (top5_ind, bot5_ind)\n",
    "                # # # bot5, bot5_ind = output_img.topk(K, 1, False, True)\n",
    "                # # print(\"Top 5 \", top5_ind)\n",
    "                # # # loss2 = avg_cos_dist(text_features[top5_ind], projections)\n",
    "                # # x1 = torch.cat((text_features[top5_ind].squeeze(), text_features[bot5_ind].squeeze()), dim=0).cuda(gpu)\n",
    "                # label = torch.tensor([1 for _ in range(K)] + [ -1 for _ in range(K)]).cuda(gpu)\n",
    "                # loss2=[]\n",
    "                # # for each in retrieved_txt_prj:\n",
    "                # #     loss2.append(criterion(x1, each.expand_as(x1), label))\n",
    "                # # print(\"Cosine embedding loss \", (sum(loss2)/len(loss2)).detach().cpu())\n",
    "                # # loss += sum(loss2)/len(loss2)\n",
    "            if which_loss in [\"both\", \"entropy\"]:\n",
    "                logit_scale = model.logit_scale.exp()\n",
    "                loss += avg_entropy(logit_scale * caption_feat @ text_features.t()) \n",
    "                print(\"Entropy loss: \", loss.detach().cpu())\n",
    "                optimizer.zero_grad() \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "    return retrieved_txt[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time_adapt_eval(val_loader, model, model_state, optimizer, optim_state, scaler, save_result=False):\n",
    "    batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f', Summary.AVERAGE)\n",
    "    top2 = AverageMeter('Acc@2', ':6.2f', Summary.AVERAGE)\n",
    "    top3 = AverageMeter('Acc@3', ':6.2f', Summary.AVERAGE)\n",
    "    top4 = AverageMeter('Acc@4', ':6.2f', Summary.AVERAGE)\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f', Summary.AVERAGE)\n",
    "    \n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, top1, top2, top3, top4, top5],\n",
    "        prefix='Test: ',\n",
    "        logger = None)\n",
    "\n",
    "    # reset model and switch to evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.reset()\n",
    "    end = time.time()\n",
    "    cnt_empty = 0\n",
    "    if save_result: Dict = dict.fromkeys(['image_path', 'caption', 'image_correct', 'caption_correct'])\n",
    "    else: Dict=None\n",
    "    \n",
    "    for i, (image, target, imagepath) in tqdm(enumerate(val_loader)): \n",
    "        assert gpu is not None\n",
    "        print(\"Image Path \", imagepath[0])\n",
    "        \n",
    "        target = target.cuda(gpu, non_blocking=True)\n",
    "        \n",
    "        ### One time training\n",
    "        # reset the tunable prompt to its initial state\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.reset()\n",
    "        optimizer.load_state_dict(optim_state)\n",
    "                \n",
    "        retrieved_caption = test_time_tuning(model, image.cuda(gpu, non_blocking=True), optimizer, scaler, imagepath[0])\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "\n",
    "                output, output_cap = model.inference(image.cuda(gpu, non_blocking=True), caption= None)\n",
    "\n",
    "        acc1, acc2, acc3, acc4, acc5 = accuracy(output, target, topk=(1, 2, 3, 4, 5), caption=None, logger=None)\n",
    "        # print(acc1, acc2, acc3, acc4, acc5)\n",
    "        top1.update(acc1[0], image.size(0))\n",
    "        top2.update(acc2[0], image.size(0))\n",
    "        top3.update(acc3[0], image.size(0))\n",
    "        top4.update(acc4[0], image.size(0))\n",
    "        top5.update(acc5[0], image.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if (i+1) % print_freq == 0:\n",
    "            progress.display(i)\n",
    "        \n",
    "\n",
    "    progress.display_summary()\n",
    "    return [top1.avg, top2.avg, top3.avg, top4.avg, top5.avg], Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##parameters\n",
    "fewshot_datasets = ['DTD', 'Flower102', 'Food101', 'Cars', 'SUN397', \n",
    "                    'Aircraft', 'Pets', 'Caltech101', 'UCF101', 'eurosat']\n",
    "arch='ViT-B/16'\n",
    "n_ctx=4\n",
    "ctx_init=\"a_photo_of_a\"\n",
    "lr = 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "if test_sets in fewshot_datasets:\n",
    "    classnames = eval(\"{}_classes\".format(test_sets.lower()))\n",
    "model = get_coop(arch, test_sets, gpu, n_ctx, ctx_init)\n",
    "model_state = None\n",
    "\n",
    "cross_check = set()\n",
    "for name, param in model.named_parameters():\n",
    "\n",
    "    if \"prompt_learner\" not in name:\n",
    "        param.requires_grad_(False)\n",
    "    if param.requires_grad : cross_check.add(name)\n",
    "print(\"tuing parameters \", cross_check)\n",
    "\n",
    "print(\"=> Model created: visual backbone {}\".format(arch))\n",
    "\n",
    "assert gpu is not None\n",
    "torch.cuda.set_device(gpu)\n",
    "model = model.cuda(gpu)\n",
    "\n",
    "trainable_param = model.prompt_learner.parameters()\n",
    "optimizer = torch.optim.AdamW(trainable_param, lr)\n",
    "optim_state = deepcopy(optimizer.state_dict())\n",
    "\n",
    "# setup automatic mixed-precision (Amp) loss scaling\n",
    "scaler = torch.cuda.amp.GradScaler(init_scale=1000)\n",
    "\n",
    "print('=> Using native Torch AMP. Training in mixed precision.')\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8041it [57:40,  2.32it/s]\n"
     ]
    }
   ],
   "source": [
    "resolution = 224\n",
    "workers = 4\n",
    "dataset_mode = 'test'\n",
    "data = '/data/seongha'\n",
    "import sys\n",
    "    # norm stats from clip.load()\n",
    "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                     std=[0.26862954, 0.26130258, 0.27577711])\n",
    "    \n",
    "    # iterating through eval datasets\n",
    "datasets = test_sets.split(\"/\")\n",
    "results = {}\n",
    "\n",
    "with open('tpt_classification_{}.txt'.format(test_sets), 'w') as f:\n",
    "    sys.stdout = f\n",
    "    for set_id in datasets:\n",
    "\n",
    "        data_transform = transforms.Compose([\n",
    "            transforms.Resize(resolution, interpolation=BICUBIC),\n",
    "            transforms.CenterCrop(resolution),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "        batchsize = 1\n",
    "        print(\"evaluating: {}\".format(set_id))\n",
    "        classnames = eval(\"{}_classes\".format(set_id.lower()))\n",
    "        model.reset_classnames(classnames, arch)\n",
    "\n",
    "        val_dataset = build_dataset(set_id, data_transform, data, mode=dataset_mode)\n",
    "        print(\"number of test samples: {}\".format(len(val_dataset)))\n",
    "\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "                    val_dataset,\n",
    "                    batch_size=batchsize, shuffle=False,\n",
    "                    num_workers=workers, pin_memory=True)\n",
    "            \n",
    "        results[set_id] = test_time_adapt_eval(val_loader, model, model_state, optimizer, optim_state, scaler)\n",
    "        del val_dataset, val_loader\n",
    "\n",
    "        try:\n",
    "            print(\"=> Acc. on testset [{}]: @1 {}/ @2 {}/ @3 {}/ @4 {}/ @5 {}\".format(set_id, results[set_id][0], results[set_id][1], results[set_id][2], results[set_id][3], results[set_id[4], results[set_id][5]]))\n",
    "        except:\n",
    "            print(\"=> Acc. on testset [{}]: {}\".format(set_id, results[set_id]))\n",
    "sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_val x (image path, caption, image_correct, caption_correct)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
