{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seongha/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "\n",
    "import time\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "import torchvision.models as models\n",
    "\n",
    "import clip\n",
    "from clip.custom_clip import get_coop\n",
    "from clip.cocoop import get_cocoop\n",
    "from data.imagnet_prompts import imagenet_classes\n",
    "from data.datautils import AugMixAugmenter, build_dataset\n",
    "from utils.tools import Summary, AverageMeter, ProgressMeter, load_model_weight, set_random_seed, create_logger\n",
    "from data.cls_to_names import *\n",
    "from data.fewshot_datasets import fewshot_datasets\n",
    "from data.imagenet_variants import thousand_k_to_200, imagenet_a_mask, imagenet_r_mask, imagenet_v_mask\n",
    "from clip_retrieval.clip_client import ClipClient, Modality\n",
    "\n",
    "from collections import defaultdict\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "client = ClipClient(\n",
    "    url=\"http://127.0.0.1:1234/knn-service\",\n",
    "    indice_name='laion_400m',\n",
    "    modality=Modality.IMAGE,\n",
    "    num_images=10,\n",
    "    deduplicate=False,\n",
    ")\n",
    "client_backup = ClipClient(\n",
    "    url=\"http://127.0.0.1:1234/knn-service\",\n",
    "    indice_name='laion_400m',\n",
    "    modality=Modality.IMAGE,\n",
    "    num_images=200,\n",
    "    deduplicate=False,\n",
    ")\n",
    "\n",
    "client_backup2 = ClipClient(\n",
    "    url=\"http://127.0.0.1:1234/knn-service\",\n",
    "    indice_name='laion_400m',\n",
    "    modality=Modality.IMAGE,\n",
    "    num_images=1000,\n",
    "    deduplicate=False,\n",
    ")\n",
    "\n",
    "#Parameters\n",
    "tta_steps = 1\n",
    "which_loss = \"cosine\"\n",
    "gpu = 7\n",
    "print_freq = 1000\n",
    "\n",
    "## Class to names mapping\n",
    "fewshot_datasets = ['DTD', 'Flower102', 'Food101', 'Cars', 'SUN397', \n",
    "                    'Aircraft', 'Pets', 'Caltech101', 'UCF101', 'eurosat']\n",
    "# test_sets = 'Caltech101/DTD/Food101'\n",
    "# test_sets= 'Cars/SUN397/Aircraft/'\n",
    "test_sets = 'Pets/UCF101/eurosat'\n",
    "\n",
    "##parameters\n",
    "arch='ViT-B/16'\n",
    "n_ctx=4\n",
    "ctx_init=\"a_photo_of_a\"\n",
    "lr = 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_confident_samples(logits, top):\n",
    "    batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "    idx = torch.argsort(batch_entropy, descending=False)[:int(batch_entropy.size()[0] * top)]\n",
    "    return logits[idx], idx\n",
    "\n",
    "def avg_entropy(outputs):\n",
    "    logits = outputs - outputs.logsumexp(dim=-1, keepdim=True) # logits = outputs.log_softmax(dim=1) [N, 1000]\n",
    "    avg_logits = logits.logsumexp(dim=0) - np.log(logits.shape[0]) # avg_logits = logits.mean(0) [1, 1000]\n",
    "    min_real = torch.finfo(avg_logits.dtype).min\n",
    "    avg_logits = torch.clamp(avg_logits, min=min_real)\n",
    "    return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy(output, target, topk=(1,), caption=None, logger=None):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "        if output.shape[0] == 1:#only image prediction\n",
    "            logit_k, pred = output.topk(maxk, 1, True, True)\n",
    "            pred = pred.t()\n",
    "            correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        else: # evaluate captions\n",
    "            bag = []\n",
    "            # # length = max(5, output.shape[0]-1)\n",
    "            # cap_pred = output[1:]\n",
    "            # # cap_pred = torch.mean(cap_pred, 0,  keepdim=True)\n",
    "            # _, pred = cap_pred.topk(maxk, 1, True, True) #5, 1 #candidate labels\n",
    "            # pred = pred.reshape(maxk, 1)\n",
    "            pred = torch.mean(output, 0, keepdim=True)\n",
    "            _, pred = pred.topk(maxk, 1, True, True)\n",
    "            pred = pred.reshape(maxk, 1)\n",
    "            correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            # if k == 1 and correct_k.item() == 0:\n",
    "\n",
    "                # pred = pred.squeeze().tolist()\n",
    "                # pred = [cls2names[lb] for lb in pred]\n",
    "\n",
    "                # if logger: logger.info(\"wrong prediction, target {} & predicted value {}\".format(target, pred))\n",
    "            # elif k==1 and correct_k.item() == 1:\n",
    "                # logger.info(\"wrong prediction , logit: \", output)\n",
    "                # pred = pred.squeeze().tolist()\n",
    "                # pred = [cls2names[lb] for lb in pred]\n",
    "\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def return_caption(img_path, retrieve_K=1):\n",
    "    try:\n",
    "        query_res = client.query(image=img_path)\n",
    "        assert len(query_res) >= retrieve_K\n",
    "        query_res = query_res[:retrieve_K]\n",
    "        retrieved_txt= [D['caption'] for D in query_res]\n",
    "        retrieved_url = [D['url'] for D in query_res]\n",
    "        retrieved_score = [D['similarity'] for D in query_res]\n",
    "        return retrieved_txt, retrieved_score\n",
    "    except:\n",
    "        query_res = client_backup2.query(image=img_path)\n",
    "        if isinstance(query_res, list) and len(query_res) >= retrieve_K:\n",
    "            query_res = query_res[:retrieve_K]\n",
    "            retrieved_txt= [D['caption'] for D in query_res]\n",
    "            retrieved_url = [D['url'] for D in query_res]\n",
    "            retrieved_score = [D['similarity'] for D in query_res]\n",
    "            return retrieved_txt, retrieved_score\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time_adapt_eval_image(val_loader, model, model_state, optimizer, optim_state, scaler, save_result=None):\n",
    "    batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f', Summary.AVERAGE)\n",
    "    top2 = AverageMeter('Acc@2', ':6.2f', Summary.AVERAGE)\n",
    "    top3 = AverageMeter('Acc@3', ':6.2f', Summary.AVERAGE)\n",
    "    top4 = AverageMeter('Acc@4', ':6.2f', Summary.AVERAGE)\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f', Summary.AVERAGE)\n",
    "    \n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, top1, top2, top3, top4, top5],\n",
    "        prefix='Test: ',\n",
    "        logger = None)\n",
    "\n",
    "    # reset model and switch to evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.reset()\n",
    "    end = time.time()\n",
    "    cnt_empty = 0\n",
    "    if save_result == None: save_result = defaultdict(list)\n",
    "    \n",
    "    for i, (image, target, imagepath) in tqdm(enumerate(val_loader)): \n",
    "        assert gpu is not None\n",
    "        # print(\"Image Path \", imagepath[0])\n",
    "        save_result['image_path'].append(imagepath[0])\n",
    "        \n",
    "        target = target.cuda(gpu, non_blocking=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                #image\n",
    "                output = model.inference(image.cuda(gpu, non_blocking=True))\n",
    "\n",
    "        # output = output / output.norm(dim=-1, keepdim=True)\n",
    "        save_result['image_entropy'].append('{:.4f}'.format(avg_entropy(output)))\n",
    "        # output = torch.nn.functional.softmax(output, dim=-1)\n",
    "        logit_k, pred = output.topk(2, 1, True, True) #1,1\n",
    "        \n",
    "        pred = pred[:,0].t()\n",
    "        logit_k= logit_k.squeeze()\n",
    "        correct = pred.eq(target)\n",
    "        # print(correct)\n",
    "        correct = correct.reshape(-1).float().sum(0, keepdim=True).item() #1 or 0\n",
    "        save_result['image_correct'].append(int(correct))\n",
    "        save_result['image_logit'].append('{:.4f}'.format(logit_k[0].item()))\n",
    "        if correct == 1:\n",
    "            # correct label - top2\n",
    "            # print(logit_k)\n",
    "            save_result['image_gap'].append('{:.4f}'.format(logit_k[0].item() - logit_k[1].item()))\n",
    "        else:\n",
    "            # incorrect top1 - real label\n",
    "            # print(target, output.squeeze()[target], logit_k )\n",
    "            save_result['image_gap'].append('{:.4f}'.format(logit_k[0].item() - output.squeeze()[target].item()))\n",
    "        \n",
    "                \n",
    "        acc1, acc2, acc3, acc4, acc5 = accuracy(output, target, topk=(1, 2, 3, 4, 5), caption=None, logger=None)\n",
    "        # print(acc1, acc2, acc3, acc4, acc5)\n",
    "        top1.update(acc1[0], image.size(0))\n",
    "        top2.update(acc2[0], image.size(0))\n",
    "        top3.update(acc3[0], image.size(0))\n",
    "        top4.update(acc4[0], image.size(0))\n",
    "        top5.update(acc5[0], image.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if (i+1) % print_freq == 0:\n",
    "            progress.display(i)\n",
    "        \n",
    "\n",
    "    progress.display_summary()\n",
    "    return [top1.avg, top2.avg, top3.avg, top4.avg, top5.avg], save_result\n",
    "\n",
    "def test_time_adapt_eval_caption(val_loader, model, model_state, optimizer, optim_state, scaler, save_result=False):\n",
    "    batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f', Summary.AVERAGE)\n",
    "    top2 = AverageMeter('Acc@2', ':6.2f', Summary.AVERAGE)\n",
    "    top3 = AverageMeter('Acc@3', ':6.2f', Summary.AVERAGE)\n",
    "    top4 = AverageMeter('Acc@4', ':6.2f', Summary.AVERAGE)\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f', Summary.AVERAGE)\n",
    "    \n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, top1, top2, top3, top4, top5],\n",
    "        prefix='Test: ',\n",
    "        logger = None)\n",
    "\n",
    "    # reset model and switch to evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.reset()\n",
    "    end = time.time()\n",
    "    cnt_empty = 0\n",
    "    assert save_result != None\n",
    "    \n",
    "    for i, (image, target, imagepath) in tqdm(enumerate(val_loader)): \n",
    "        assert gpu is not None\n",
    "        # print(\"Image Path \", imagepath[0])\n",
    "        # save_result['image_path'] = imagepath[0]\n",
    "        \n",
    "        target = target.cuda(gpu, non_blocking=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                #caption\n",
    "                retrieved_Caption, _ = return_caption(imagepath[0])\n",
    "                if retrieved_Caption==None:\n",
    "                    cnt_empty +=1\n",
    "                    save_result['caption'].append(\"None\")\n",
    "                    save_result['caption_entropy'].append(np.nan)\n",
    "                    save_result['caption_correct'].append(np.nan)\n",
    "                    save_result['caption_logit'].append(np.nan)\n",
    "                    save_result['caption_gap'].append(np.nan)\n",
    "                    continue\n",
    "                save_result['caption'].append(retrieved_Caption[0])\n",
    "                output_caption = model.caption_ensemble(retrieved_Caption)\n",
    "\n",
    "        # output_caption = output_caption / output_caption.norm(dim=-1, keepdim=True)\n",
    "        save_result['caption_entropy'].append('{:.4f}'.format(avg_entropy(output_caption)))\n",
    "        # print(output_caption.shape)\n",
    "        # output_caption = torch.nn.functional.softmax(output_caption, dim=-1)\n",
    "        # print(output_caption)\n",
    "        logit_k, pred = output_caption.topk(2, 1, True, True) #1,2\n",
    "        # print(logit_k, pred)\n",
    "\n",
    "        logit_k = logit_k.squeeze()\n",
    "        if logit_k[0] == 1:\n",
    "            print(output_caption)\n",
    "        \n",
    "        pred = pred[:,0].t()\n",
    "        correct = pred.eq(target)\n",
    "        correct = correct.reshape(-1).float().sum(0, keepdim=True).item() #1 or 0\n",
    "        save_result['caption_correct'].append(int(correct))\n",
    "        save_result['caption_logit'].append('{:.4f}'.format(logit_k[0].item()))\n",
    "        if correct == 1:\n",
    "            # correct label - top2\n",
    "            # print(logit_k)\n",
    "            save_result['caption_gap'].append('{:.4f}'.format(logit_k[0].item() - logit_k[1].item()))\n",
    "        else:\n",
    "            # incorrect top1 - real label\n",
    "            # print(target, output.squeeze()[target], logit_k )\n",
    "            save_result['caption_gap'].append('{:.4f}'.format(logit_k[0].item() - output_caption.squeeze()[target].item()))\n",
    "        \n",
    "         \n",
    "        acc1, acc2, acc3, acc4, acc5 = accuracy(output_caption, target, topk=(1, 2, 3, 4, 5), caption=None, logger=None)\n",
    "        # print(acc1, acc2, acc3, acc4, acc5)\n",
    "        top1.update(acc1[0], image.size(0))\n",
    "        top2.update(acc2[0], image.size(0))\n",
    "        top3.update(acc3[0], image.size(0))\n",
    "        top4.update(acc4[0], image.size(0))\n",
    "        top5.update(acc5[0], image.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if (i+1) % print_freq == 0:\n",
    "            progress.display(i)\n",
    "    \n",
    "    print(\"empty caption count = {}\".format(cnt_empty))\n",
    "    progress.display_summary()\n",
    "    return [top1.avg, top2.avg, top3.avg, top4.avg, top5.avg], save_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype  torch.float32\n",
      "Initializing the contect with given words: [a_photo_of_a]\n",
      "Initial context: \"a photo of a\"\n",
      "Number of context words (tokens): 4\n",
      "tuing parameters  {'prompt_learner.ctx'}\n",
      "=> Model created: visual backbone ViT-B/16\n",
      "=> Using native Torch AMP. Training in mixed precision.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "if test_sets in fewshot_datasets:\n",
    "    classnames = eval(\"{}_classes\".format(test_sets.lower()))\n",
    "model = get_coop(arch, test_sets, gpu, n_ctx, ctx_init)\n",
    "model_state = None\n",
    "\n",
    "cross_check = set()\n",
    "for name, param in model.named_parameters():\n",
    "\n",
    "    if \"prompt_learner\" not in name:\n",
    "        param.requires_grad_(False)\n",
    "    if param.requires_grad : cross_check.add(name)\n",
    "print(\"tuing parameters \", cross_check)\n",
    "\n",
    "print(\"=> Model created: visual backbone {}\".format(arch))\n",
    "\n",
    "assert gpu is not None\n",
    "torch.cuda.set_device(gpu)\n",
    "model = model.cuda(gpu)\n",
    "\n",
    "trainable_param = model.prompt_learner.parameters()\n",
    "optimizer = torch.optim.AdamW(trainable_param, lr)\n",
    "optim_state = deepcopy(optimizer.state_dict())\n",
    "\n",
    "# setup automatic mixed-precision (Amp) loss scaling\n",
    "scaler = torch.cuda.amp.GradScaler(init_scale=1000)\n",
    "\n",
    "print('=> Using native Torch AMP. Training in mixed precision.')\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_max_accuracy(df, total_length):\n",
    "    img_ent_m = df['image_entropy'].astype('float').mean()\n",
    "    cap_ent_m = df['caption_entropy'].astype('float').mean()\n",
    "    print(\"image ent mean  {} caption ent mean {}\".format(img_ent_m,cap_ent_m ))\n",
    "    df['image_entropy'] = df['image_entropy'].astype('float')\n",
    "    df['caption_entropy'] = df['caption_entropy'].astype('float')\n",
    "\n",
    "\n",
    "    table = dict()\n",
    "    for c_tau in range(0, 1):\n",
    "        for i_tau in range(0, 1):\n",
    "            c_tau *= 0.01\n",
    "            i_tau *= 0.01\n",
    "            # case1\n",
    "            pred_cap = df.loc[(df.image_entropy > img_ent_m * (1+i_tau)) & (df.caption_entropy < cap_ent_m * (1-c_tau))]\n",
    "            pred_cap_n = pred_cap.caption_correct.sum()\n",
    "            \n",
    "            \n",
    "            # case2\n",
    "            pred_img = df.loc[(df.image_entropy <= img_ent_m * (1+i_tau)) | (df.caption_entropy >= cap_ent_m * (1-c_tau))]\n",
    "            # assert pred_cap.shape[0]+ pred_img.shape[0] == total_length, ( pred_cap.shape[0]+ pred_img.shape[0])\n",
    "            pred_img_n = pred_img.image_correct.sum()\n",
    "            max_acc = (pred_cap_n+pred_img_n)/total_length\n",
    "            table[(c_tau, i_tau, pred_cap_n)] = max_acc\n",
    "            \n",
    "            if i_tau == 0 and c_tau == 0:\n",
    "                print(\"pred cap shape {} out of {}\".format(pred_cap_n, pred_cap.shape))\n",
    "                print(\"default {:2f}\".format(max_acc * 100))\n",
    "    max_key = max(table, key=table.get)\n",
    "    # print(max_key, table[max_key])\n",
    "    return (max_key, table[max_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating: Flower102\n",
      "number of test samples: 2463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1005it [00:34, 29.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 999/2463]\tTime  0.038 ( 0.034)\tAcc@1 100.00 ( 66.80)\tAcc@2 100.00 ( 79.80)\tAcc@3 100.00 ( 82.70)\tAcc@4 100.00 ( 84.30)\tAcc@5 100.00 ( 85.80)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2005it [01:08, 29.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [1999/2463]\tTime  0.033 ( 0.034)\tAcc@1 100.00 ( 66.70)\tAcc@2 100.00 ( 79.05)\tAcc@3 100.00 ( 82.20)\tAcc@4 100.00 ( 83.80)\tAcc@5 100.00 ( 84.95)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2463it [01:24, 29.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *  Acc@1 67.032 Acc@2 79.375 Acc@3 82.217 Acc@4 83.719 Acc@5 84.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1000it [09:18,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 999/2463]\tTime  0.255 ( 0.559)\tAcc@1 100.00 ( 42.40)\tAcc@2 100.00 ( 50.60)\tAcc@3 100.00 ( 54.00)\tAcc@4 100.00 ( 56.70)\tAcc@5 100.00 ( 58.40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [18:30,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [1999/2463]\tTime  0.217 ( 0.555)\tAcc@1 100.00 ( 43.35)\tAcc@2 100.00 ( 50.65)\tAcc@3 100.00 ( 54.45)\tAcc@4 100.00 ( 57.25)\tAcc@5 100.00 ( 58.75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2463it [22:35,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty caption count = 0\n",
      " *  Acc@1 43.159 Acc@2 50.792 Acc@3 54.365 Acc@4 57.328 Acc@5 58.831\n",
      "img_diff (943, 10)\n",
      "Image Entropy mean 0.7874897136797455\n",
      "Caption Entropy mean 0.5247625662778367\n",
      "\n",
      "cap_diff (355, 10)\n",
      "Image Entropy mean 1.75411661971831\n",
      "Caption Entropy mean 0.09797267605633803\n",
      "\n",
      "intersection (709, 10)\n",
      "Image Entropy mean 0.783781946403385\n",
      "Caption Entropy mean 0.11586713681241184\n",
      "\n",
      "neither (456, 10)\n",
      "Image Entropy mean 1.7897495614035088\n",
      "Caption Entropy mean 0.5187515350877193\n",
      "\n",
      "union (2007, 10)\n",
      "Image Entropy mean 0.9571577478824115\n",
      "Caption Entropy mean 0.3048237169905331\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "resolution = 224\n",
    "workers = 4\n",
    "dataset_mode = 'test'\n",
    "data = '/data/seongha'\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "    # norm stats from clip.load()\n",
    "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                     std=[0.26862954, 0.26130258, 0.27577711])\n",
    "    \n",
    "    # iterating through eval datasets\n",
    "datasets = test_sets.split(\"/\")\n",
    "results = {}\n",
    "\n",
    "# with open('inferece_image_caption{}.txt'.format(test_sets), 'w') as f:\n",
    "    # sys.stdout = f\n",
    "\n",
    "\n",
    "for set_id in datasets:\n",
    "    Dict = defaultdict(list)\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize(resolution, interpolation=BICUBIC),\n",
    "        transforms.CenterCrop(resolution),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    batchsize = 1\n",
    "    print(\"evaluating: {}\".format(set_id))\n",
    "    classnames = eval(\"{}_classes\".format(set_id.lower()))\n",
    "    model.reset_classnames(classnames, arch)\n",
    "\n",
    "    val_dataset = build_dataset(set_id, data_transform, data, mode=dataset_mode)\n",
    "    total_length = len(val_dataset)\n",
    "    print(\"number of test samples: {}\".format(len(val_dataset)))\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=batchsize, shuffle=True,\n",
    "                num_workers=workers, pin_memory=True)\n",
    "        \n",
    "    results['image'], tmp = test_time_adapt_eval_image(val_loader, model, model_state, optimizer, optim_state, scaler, Dict)\n",
    "    Dict = tmp\n",
    "    results['caption'], tmp = test_time_adapt_eval_caption(val_loader, model, model_state, optimizer, optim_state, scaler, Dict)\n",
    "    Dict = tmp\n",
    "    # assert len(Dict['image_path']) == len(Dict['caption']) and len(Dict['image_correct']) == len(Dict['caption_correct']), [len(v) for k, v in Dict.items()]\n",
    "    del val_dataset, val_loader\n",
    "\n",
    "    # try:\n",
    "    #     print(\"=> Acc. on testset [{}]: @1 {}/ @2 {}/ @3 {}/ @4 {}/ @5 {}\".format(set_id, results[set_id][0], results[set_id][1], results['image'][2], results['image'][3], results['image'[4], results['image'][5]]))\n",
    "    # except:\n",
    "    #     print(\"=> Acc. on testset [{}]: {}\".format(set_id, results[set_id]))\n",
    "# sys.stdout = sys.__stdout__\n",
    "    tmp = {k: v for k, v in Dict.items() if \"image\" in k}\n",
    "    df_img = pd.DataFrame(tmp)\n",
    "    df_img = df_img.reset_index()\n",
    "\n",
    "    tmp = {k: v for k, v in Dict.items() if \"caption\" in k}\n",
    "    df_cap = pd.DataFrame(tmp)\n",
    "    df_cap = df_cap.reset_index()\n",
    "\n",
    "    path = './notebook/inference_image_caption'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    df = pd.DataFrame(Dict)\n",
    "    df.to_csv(os.path.join(path, 'inference_image_caption_{}.csv'.format(set_id)))\n",
    "\n",
    "    with open(os.path.join(path,'inference_image_caption_{}.txt'.format(set_id)), 'w') as f:\n",
    "        \n",
    "        img_corr_ind = df_img.loc[df_img['image_correct'] == 1, 'index'].to_list()\n",
    "        cap_corr_ind = df_cap.loc[df_cap['caption_correct'] == 1, 'index'].to_list()\n",
    "        neither = set(df_img.loc[df_img['image_correct'] == 0, 'index'].to_list()) & set(df_cap.loc[df_cap['caption_correct'] == 0, 'index'].to_list())\n",
    "        #image accuracy, caption ensemble accuracy\n",
    "        f.write(\"1. Image accuracy {:.4f}, Caption Accuracy {:.4f}\\n\".format( len(img_corr_ind)/total_length, len(cap_corr_ind)/total_length))\n",
    "        # 4 cases\n",
    "        union = set(img_corr_ind) | set(cap_corr_ind)\n",
    "        f.write(\"2: 4 cases\\n\")\n",
    "        f.write(\"Union :{}\\n\".format( len(union)))\n",
    "        intersection = set(img_corr_ind) & set(cap_corr_ind)\n",
    "        f.write(\"intersection {}\\n\".format( len(intersection)))\n",
    "        img_diff = set(img_corr_ind) - set(cap_corr_ind)\n",
    "        f.write(\"image only {}\\n\".format(len(img_diff)))\n",
    "        cap_diff = set(cap_corr_ind) - set(img_corr_ind)\n",
    "        f.write(\"cap only {}\\n\".format( len(cap_diff)))\n",
    "        f.write(\"neither {}\\n\".format( len(neither)))\n",
    "        # max accuracy\n",
    "        f.write(\"3. Max accuracy\\n\")\n",
    "        f.write(\"Max accuracy: {:.4f}\\n\".format(len(union)/total_length*100)) \n",
    "        #entropy, logit gap\n",
    "        f.write(\"4. Entropy & Logit Gap\\n\")\n",
    "        f.write(\"Image\\n\")\n",
    "        img_correct = df_img.loc[df_img['image_correct'] == 1]\n",
    "        f.write(\"correct\\n\")\n",
    "        f.write(\" {}\\n\".format(str(img_correct.shape)))\n",
    "        f.write(\"top1 - top2 mean, std\\n\")\n",
    "        f.write(\"{} {}\\n\".format(img_correct['image_gap'].astype(float).mean(), img_correct['image_gap'].astype(float).std() ))\n",
    "        f.write(\"Entropy mean {}\\n\".format(str(img_correct['image_entropy'].astype(float).mean())))\n",
    "        f.write(\"\")\n",
    "        img_wrong = df_img.loc[df_img['image_correct'] == 0]\n",
    "        f.write(\"wrong\\n\")\n",
    "        f.write(\" {}\\n\".format(str(img_wrong.shape)))\n",
    "        f.write(\"pred(top1) - target mean, std\\n\")\n",
    "        f.write(\"{} {}\\n\".format(img_wrong['image_gap'].astype(float).mean(), img_wrong['image_gap'].astype(float).std() ))\n",
    "        f.write(\"Entropy mean {}\\n\".format(str(img_wrong['image_entropy'].astype(float).mean())))\n",
    "        f.write(\"-\"*10 + \"\\n\")\n",
    "        \n",
    "        f.write(\"Caption\\n\")\n",
    "        cap_correct = df_cap.loc[df_cap['caption_correct'] == 1]\n",
    "        f.write(\"correct\\n\")\n",
    "        f.write(\" {}\\n\".format(str(cap_correct.shape)))\n",
    "        f.write(\"top1 - top2 mean, std\\n\")\n",
    "        f.write(\"{} {}\\n\".format(cap_correct['caption_gap'].astype(float).mean(), cap_correct['caption_gap'].astype(float).std() ))\n",
    "        f.write(\"Entropy mean {}\\n\".format(str(cap_correct['caption_entropy'].astype(float).mean())))\n",
    "        f.write(\"\")\n",
    "        cap_wrong = df_cap.loc[df_cap['caption_correct'] == 0]\n",
    "        f.write(\"wrong\\n\")\n",
    "        f.write(\" {}\\n\".format(str(cap_wrong.shape)))\n",
    "        f.write(\"pred(top1) - target mean, std\\n\")\n",
    "        f.write(\"{} {}\\n\".format(cap_wrong['caption_gap'].astype(float).mean(), cap_wrong['caption_gap'].astype(float).std() ))\n",
    "        f.write(\"Entropy mean {}\\n\".format(str(cap_wrong['caption_entropy'].astype(float).mean())))\n",
    "            \n",
    "\n",
    "        for i_set, i_name in zip([img_diff, cap_diff, intersection, neither, union], ['img_diff', 'cap_diff', 'intersection', 'neither', 'union']):\n",
    "            df_ = df.iloc[list(i_set)]\n",
    "            f.write(\"{} {}\\n\".format(i_name, df_.shape))\n",
    "            f.write(\"Image Entropy mean {}\\n\".format(str(df_['image_entropy'].astype(float).mean())))\n",
    "            f.write(\"Caption Entropy mean {}\\n\".format(str(df_['caption_entropy'].astype(float).mean())))\n",
    "        print(\"how many nan values ? {}\\n\".format(df.loc[cap_diff].isna().sum()))\n",
    "        f.write(\"how many nan values ? {}\\n\".format(df.loc[cap_diff].isna().sum()))\n",
    "        (c_tau, i_tau, _), max_acc = get_max_accuracy( df, total_length)\n",
    "        print(\"Caption tau {}, image tau {}, max acc {:.4f}\\n\".format(c_tau, i_tau, max_acc))\n",
    "        f.write(\"Caption tau {}, image tau {}, max acc {:.4f}\\n\".format(c_tau, i_tau, max_acc))\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTD\n",
      "1692\n",
      "caption entropy shape (1692,)\n",
      "image ent mean  1.7448628250591016 caption ent mean 1.1990325650118203\n",
      "pred cap shape 201 out of (460, 11)\n",
      "default 50.118203\n",
      "Caption tau 0.0, image tau 0.0, max acc 50.1182\n",
      "pred cap n 201\n",
      "\n",
      "Flower102\n",
      "2463\n",
      "caption entropy shape (2463,)\n",
      "image ent mean  1.1113038570848561 caption ent mean 0.34443032886723507\n",
      "pred cap shape 424 out of (778, 11)\n",
      "default 70.889160\n",
      "Caption tau 0.0, image tau 0.0, max acc 70.8892\n",
      "pred cap n 424\n",
      "\n",
      "Food101\n",
      "30300\n",
      "caption entropy shape (30300,)\n",
      "image ent mean  0.5977752244224422 caption ent mean 0.44765492260470646\n",
      "pred cap shape 4683.0 out of (7591, 11)\n",
      "default 83.326733\n",
      "Caption tau 0.0, image tau 0.0, max acc 83.3267\n",
      "pred cap n 4683.0\n",
      "\n",
      "Cars\n",
      "8041\n",
      "caption entropy shape (8041,)\n",
      "image ent mean  1.1570252083074244 caption ent mean 0.5936343365253077\n",
      "pred cap shape 1041 out of (2262, 11)\n",
      "default 65.563985\n",
      "Caption tau 0.0, image tau 0.0, max acc 65.5640\n",
      "pred cap n 1041\n",
      "\n",
      "SUN397\n",
      "19850\n",
      "caption entropy shape (19850,)\n",
      "image ent mean  1.429165748110831 caption ent mean 1.0612905783958082\n",
      "pred cap shape 2628.0 out of (5341, 11)\n",
      "default 64.166247\n",
      "Caption tau 0.0, image tau 0.0, max acc 64.1662\n",
      "pred cap n 2628.0\n",
      "\n",
      "Aircraft\n",
      "3333\n",
      "caption entropy shape (3333,)\n",
      "image ent mean  2.449992409240924 caption ent mean 1.4639754575457549\n",
      "pred cap shape 219 out of (1111, 11)\n",
      "default 25.592559\n",
      "Caption tau 0.0, image tau 0.0, max acc 25.5926\n",
      "pred cap n 219\n",
      "\n",
      "Pets\n",
      "3669\n",
      "caption entropy shape (3669,)\n",
      "image ent mean  0.4508049877350777 caption ent mean 0.4083931025081789\n",
      "pred cap shape 501.0 out of (944, 11)\n",
      "default 83.346961\n",
      "Caption tau 0.0, image tau 0.0, max acc 83.3470\n",
      "pred cap n 501.0\n",
      "\n",
      "Caltech101\n",
      "2465\n",
      "caption entropy shape (2465,)\n",
      "image ent mean  0.48654620689655176 caption ent mean 0.40539006085192697\n",
      "pred cap shape 539 out of (601, 11)\n",
      "default 94.604462\n",
      "Caption tau 0.0, image tau 0.0, max acc 94.6045\n",
      "pred cap n 539\n",
      "\n",
      "UCF101\n",
      "3783\n",
      "caption entropy shape (3783,)\n",
      "image ent mean  1.1459249273063705 caption ent mean 0.8475289578361176\n",
      "pred cap shape 562.0 out of (982, 11)\n",
      "default 69.838752\n",
      "Caption tau 0.0, image tau 0.0, max acc 69.8388\n",
      "pred cap n 562.0\n",
      "\n",
      "eurosat\n",
      "8100\n",
      "caption entropy shape (8100,)\n",
      "image ent mean  1.4223896543209877 caption ent mean 1.44657512345679\n",
      "pred cap shape 420 out of (2158, 11)\n",
      "default 39.333333\n",
      "Caption tau 0.0, image tau 0.0, max acc 39.3333\n",
      "pred cap n 420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "di = dict()\n",
    "for each in fewshot_datasets:\n",
    "    print(each)\n",
    "    path = '/home/seongha/TPT/notebook/inference_image_caption/{}/inference_image_caption_{}.csv'.format(arch.replace('/', ''), each)\n",
    "    df = pd.read_csv(path)\n",
    "    total_length = df.shape[0]\n",
    "    print(total_length)\n",
    "    print(\"caption entropy shape {}\".format(df['caption_entropy'].shape))\n",
    "    (c_tau, i_tau, pred_cap_n), max_acc = get_max_accuracy(df, total_length)\n",
    "    print(\"Caption tau {}, image tau {}, max acc {:.4f}\".format(c_tau, i_tau, max_acc * 100))\n",
    "    print(\"pred cap n {}\\n\".format(pred_cap_n))\n",
    "    di[each] = {'i_tau': i_tau, 'c_tau': c_tau}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2829, 11)\n",
      "(528, 11)\n",
      "(738, 11)\n",
      "(4005, 11)\n"
     ]
    }
   ],
   "source": [
    "Path = \"./notebook/inference_image_caption/ViT-B16/inference_image_caption_eurosat.csv\"\n",
    "import pandas as pd\n",
    "df = pd.read_csv(Path)\n",
    "\n",
    "for a, b in zip([1,1,0,0], [0,1,1,0]):\n",
    "    print(df.loc[(df.image_correct == a) & (df.caption_correct == b)].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
