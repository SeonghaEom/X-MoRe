{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seongha/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "\n",
    "import time\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "import torchvision.models as models\n",
    "\n",
    "import clip\n",
    "from clip.custom_clip import get_coop\n",
    "from clip.cocoop import get_cocoop\n",
    "from data.imagnet_prompts import imagenet_classes\n",
    "from data.datautils import AugMixAugmenter, build_dataset\n",
    "from utils.tools import Summary, AverageMeter, ProgressMeter, load_model_weight, set_random_seed, create_logger\n",
    "from data.cls_to_names import *\n",
    "from data.fewshot_datasets import fewshot_datasets\n",
    "from data.imagenet_variants import thousand_k_to_200, imagenet_a_mask, imagenet_r_mask, imagenet_v_mask\n",
    "from clip_retrieval.clip_client import ClipClient, Modality\n",
    "\n",
    "from collections import defaultdict\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "client = ClipClient(\n",
    "    url=\"http://127.0.0.1:1234/knn-service\",\n",
    "    indice_name='laion_400m',\n",
    "    modality=Modality.IMAGE,\n",
    "    num_images=10,\n",
    "    deduplicate=False,\n",
    ")\n",
    "client_backup = ClipClient(\n",
    "    url=\"http://127.0.0.1:1234/knn-service\",\n",
    "    indice_name='laion_400m',\n",
    "    modality=Modality.IMAGE,\n",
    "    num_images=200,\n",
    "    deduplicate=False,\n",
    ")\n",
    "\n",
    "client_backup2 = ClipClient(\n",
    "    url=\"http://127.0.0.1:1234/knn-service\",\n",
    "    indice_name='laion_400m',\n",
    "    modality=Modality.IMAGE,\n",
    "    num_images=1000,\n",
    "    deduplicate=False,\n",
    ")\n",
    "\n",
    "## Class to names mapping\n",
    "fewshot_datasets = ['DTD', 'Flower102', 'Food101', 'Cars', 'SUN397', \n",
    "                    'Aircraft', 'Pets', 'Caltech101', 'UCF101', 'eurosat']\n",
    "test_sets = 'Caltech101'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_confident_samples(logits, top):\n",
    "    batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "    idx = torch.argsort(batch_entropy, descending=False)[:int(batch_entropy.size()[0] * top)]\n",
    "    return logits[idx], idx\n",
    "\n",
    "def avg_entropy(outputs):\n",
    "    # epsilon = 1e-10\n",
    "    assert len(outputs) > 0\n",
    "    assert torch.any(torch.isnan(outputs)) == False\n",
    "    logits = outputs - outputs.logsumexp(dim=-1, keepdim=True) # logits = outputs.log_softmax(dim=1) [N, 1000]\n",
    "    assert torch.any(torch.isnan(logits)) == False\n",
    "    avg_logits = logits.logsumexp(dim=0) - np.log(logits.shape[0]) # avg_logits = logits.mean(0) [1, 1000]\n",
    "    # print(avg_logits)\n",
    "    if torch.any(torch.isnan(avg_logits)):\n",
    "        print(\"average logits \", outputs.log_softmax(dim=1).mean(0))\n",
    "    assert torch.any(torch.isnan(avg_logits)) == False\n",
    "    \n",
    "    min_real = torch.finfo(avg_logits.dtype).min\n",
    "    avg_logits = torch.clamp(avg_logits, min=min_real)\n",
    "    assert torch.any(torch.isnan(avg_logits)) == False\n",
    "    return -((avg_logits) * (torch.exp(avg_logits))).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy(output, target, topk=(1,), caption=None, logger=None):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "        if output.shape[0] == 1:#only image prediction\n",
    "            logit_k, pred = output.topk(maxk, 1, True, True)\n",
    "            pred = pred.t()\n",
    "            correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        else: # evaluate captions\n",
    "            bag = []\n",
    "            # # length = max(5, output.shape[0]-1)\n",
    "            # cap_pred = output[1:]\n",
    "            # # cap_pred = torch.mean(cap_pred, 0,  keepdim=True)\n",
    "            # _, pred = cap_pred.topk(maxk, 1, True, True) #5, 1 #candidate labels\n",
    "            # pred = pred.reshape(maxk, 1)\n",
    "            pred = torch.mean(output, 0, keepdim=True)\n",
    "            _, pred = pred.topk(maxk, 1, True, True)\n",
    "            pred = pred.reshape(maxk, 1)\n",
    "            correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            # if k == 1 and correct_k.item() == 0:\n",
    "\n",
    "                # pred = pred.squeeze().tolist()\n",
    "                # pred = [cls2names[lb] for lb in pred]\n",
    "\n",
    "                # if logger: logger.info(\"wrong prediction, target {} & predicted value {}\".format(target, pred))\n",
    "            # elif k==1 and correct_k.item() == 1:\n",
    "                # logger.info(\"wrong prediction , logit: \", output)\n",
    "                # pred = pred.squeeze().tolist()\n",
    "                # pred = [cls2names[lb] for lb in pred]\n",
    "\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "tta_steps = 1\n",
    "which_loss = \"cosine\"\n",
    "gpu = 7\n",
    "print_freq = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def return_caption(img_path, retrieve_K=1):\n",
    "    try:\n",
    "        query_res = client.query(image=img_path)\n",
    "        assert len(query_res) >= retrieve_K\n",
    "        query_res = query_res[:retrieve_K]\n",
    "        retrieved_txt= [D['caption'] for D in query_res]\n",
    "        retrieved_url = [D['url'] for D in query_res]\n",
    "        retrieved_score = [D['similarity'] for D in query_res]\n",
    "        return retrieved_txt, retrieved_score\n",
    "    except:\n",
    "        query_res = client_backup2.query(image=img_path)\n",
    "        if len(query_res) >= retrieve_K:\n",
    "            query_res = query_res[:retrieve_K]\n",
    "            retrieved_txt= [D['caption'] for D in query_res]\n",
    "            retrieved_url = [D['url'] for D in query_res]\n",
    "            retrieved_score = [D['similarity'] for D in query_res]\n",
    "            return retrieved_txt, retrieved_score\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time_adapt_eval_image(val_loader, model, model_state, optimizer, optim_state, scaler, save_result=None):\n",
    "    batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f', Summary.AVERAGE)\n",
    "    top2 = AverageMeter('Acc@2', ':6.2f', Summary.AVERAGE)\n",
    "    top3 = AverageMeter('Acc@3', ':6.2f', Summary.AVERAGE)\n",
    "    top4 = AverageMeter('Acc@4', ':6.2f', Summary.AVERAGE)\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f', Summary.AVERAGE)\n",
    "    \n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, top1, top2, top3, top4, top5],\n",
    "        prefix='Test: ',\n",
    "        logger = None)\n",
    "\n",
    "    # reset model and switch to evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.reset()\n",
    "    end = time.time()\n",
    "    cnt_empty = 0\n",
    "    if save_result == None: save_result = defaultdict(list)\n",
    "    \n",
    "    for i, (image, target, imagepath) in tqdm(enumerate(val_loader)): \n",
    "        assert gpu is not None\n",
    "        # print(\"Image Path \", imagepath[0])\n",
    "        save_result['image_path'].append(imagepath[0])\n",
    "        \n",
    "        target = target.cuda(gpu, non_blocking=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                #image\n",
    "                output = model.inference(image.cuda(gpu, non_blocking=True))\n",
    "\n",
    "        # output = output / output.norm(dim=-1, keepdim=True)\n",
    "        save_result['image_entropy'].append('{:.4f}'.format(avg_entropy(output)))\n",
    "        output = torch.nn.functional.softmax(output, dim=-1)\n",
    "        logit_k, pred = output.topk(2, 1, True, True) #1,1\n",
    "        \n",
    "        pred = pred[:,0].t()\n",
    "        logit_k= logit_k.squeeze()\n",
    "        correct = pred.eq(target)\n",
    "        # print(correct)\n",
    "        correct = correct.reshape(-1).float().sum(0, keepdim=True).item() #1 or 0\n",
    "        save_result['image_correct'].append(int(correct))\n",
    "        save_result['image_logit'].append('{:.4f}'.format(logit_k[0].item()))\n",
    "        if correct == 1:\n",
    "            # correct label - top2\n",
    "            # print(logit_k)\n",
    "            save_result['image_gap'].append('{:.4f}'.format(logit_k[0].item() - logit_k[1].item()))\n",
    "        else:\n",
    "            # incorrect top1 - real label\n",
    "            # print(target, output.squeeze()[target], logit_k )\n",
    "            save_result['image_gap'].append('{:.4f}'.format(logit_k[0].item() - output.squeeze()[target].item()))\n",
    "        \n",
    "                \n",
    "        acc1, acc2, acc3, acc4, acc5 = accuracy(output, target, topk=(1, 2, 3, 4, 5), caption=None, logger=None)\n",
    "        # print(acc1, acc2, acc3, acc4, acc5)\n",
    "        top1.update(acc1[0], image.size(0))\n",
    "        top2.update(acc2[0], image.size(0))\n",
    "        top3.update(acc3[0], image.size(0))\n",
    "        top4.update(acc4[0], image.size(0))\n",
    "        top5.update(acc5[0], image.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if (i+1) % print_freq == 0:\n",
    "            progress.display(i)\n",
    "        \n",
    "\n",
    "    progress.display_summary()\n",
    "    return [top1.avg, top2.avg, top3.avg, top4.avg, top5.avg], save_result\n",
    "\n",
    "def test_time_adapt_eval_caption(val_loader, model, model_state, optimizer, optim_state, scaler, save_result=False):\n",
    "    batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f', Summary.AVERAGE)\n",
    "    top2 = AverageMeter('Acc@2', ':6.2f', Summary.AVERAGE)\n",
    "    top3 = AverageMeter('Acc@3', ':6.2f', Summary.AVERAGE)\n",
    "    top4 = AverageMeter('Acc@4', ':6.2f', Summary.AVERAGE)\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f', Summary.AVERAGE)\n",
    "    \n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, top1, top2, top3, top4, top5],\n",
    "        prefix='Test: ',\n",
    "        logger = None)\n",
    "\n",
    "    # reset model and switch to evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.reset()\n",
    "    end = time.time()\n",
    "    cnt_empty = 0\n",
    "    assert save_result != None\n",
    "    \n",
    "    for i, (image, target, imagepath) in tqdm(enumerate(val_loader)): \n",
    "        assert gpu is not None\n",
    "        # print(\"Image Path \", imagepath[0])\n",
    "        # save_result['image_path'] = imagepath[0]\n",
    "        \n",
    "        target = target.cuda(gpu, non_blocking=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                #caption\n",
    "                retrieved_Caption, _ = return_caption(imagepath[0])\n",
    "                if retrieved_Caption==None:\n",
    "                    cnt_empty +=1\n",
    "                    continue\n",
    "                save_result['caption'].append(retrieved_Caption[0])\n",
    "                output_caption = model.caption_ensemble(retrieved_Caption)\n",
    "\n",
    "        # output_caption = output_caption / output_caption.norm(dim=-1, keepdim=True)\n",
    "        save_result['caption_entropy'].append('{:.4f}'.format(avg_entropy(output_caption)))\n",
    "        output_caption = torch.nn.functional.softmax(output_caption, dim=-1)\n",
    "        logit_k, pred = output_caption.topk(2, 1, True, True) #1,2\n",
    "        logit_k = logit_k.squeeze()\n",
    "        \n",
    "        pred = pred[:,0].t()\n",
    "        correct = pred.eq(target)\n",
    "        correct = correct.reshape(-1).float().sum(0, keepdim=True).item() #1 or 0\n",
    "        save_result['caption_correct'].append(int(correct))\n",
    "        save_result['caption_logit'].append('{:.4f}'.format(logit_k[0].item()))\n",
    "        if correct == 1:\n",
    "            # correct label - top2\n",
    "            # print(logit_k)\n",
    "            save_result['caption_gap'].append('{:.4f}'.format(logit_k[0].item() - logit_k[1].item()))\n",
    "        else:\n",
    "            # incorrect top1 - real label\n",
    "            # print(target, output.squeeze()[target], logit_k )\n",
    "            save_result['caption_gap'].append('{:.4f}'.format(logit_k[0].item() - output_caption.squeeze()[target].item()))\n",
    "        \n",
    "         \n",
    "        acc1, acc2, acc3, acc4, acc5 = accuracy(output_caption, target, topk=(1, 2, 3, 4, 5), caption=None, logger=None)\n",
    "        # print(acc1, acc2, acc3, acc4, acc5)\n",
    "        top1.update(acc1[0], image.size(0))\n",
    "        top2.update(acc2[0], image.size(0))\n",
    "        top3.update(acc3[0], image.size(0))\n",
    "        top4.update(acc4[0], image.size(0))\n",
    "        top5.update(acc5[0], image.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if (i+1) % print_freq == 0:\n",
    "            progress.display(i)\n",
    "    \n",
    "    print(\"empty caption count = {}\".format(cnt_empty))\n",
    "    progress.display_summary()\n",
    "    return [top1.avg, top2.avg, top3.avg, top4.avg, top5.avg], save_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##parameters\n",
    "fewshot_datasets = ['DTD', 'Flower102', 'Food101', 'Cars', 'SUN397', \n",
    "                    'Aircraft', 'Pets', 'Caltech101', 'UCF101', 'eurosat']\n",
    "arch='ViT-B/16'\n",
    "n_ctx=4\n",
    "ctx_init=\"a_photo_of_a\"\n",
    "lr = 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the contect with given words: [a_photo_of_a]\n",
      "Initial context: \"a photo of a\"\n",
      "Number of context words (tokens): 4\n",
      "tuing parameters  {'prompt_learner.ctx'}\n",
      "=> Model created: visual backbone ViT-B/16\n",
      "=> Using native Torch AMP. Training in mixed precision.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "if test_sets in fewshot_datasets:\n",
    "    classnames = eval(\"{}_classes\".format(test_sets.lower()))\n",
    "model = get_coop(arch, test_sets, gpu, n_ctx, ctx_init)\n",
    "model_state = None\n",
    "\n",
    "cross_check = set()\n",
    "for name, param in model.named_parameters():\n",
    "\n",
    "    if \"prompt_learner\" not in name:\n",
    "        param.requires_grad_(False)\n",
    "    if param.requires_grad : cross_check.add(name)\n",
    "print(\"tuing parameters \", cross_check)\n",
    "\n",
    "print(\"=> Model created: visual backbone {}\".format(arch))\n",
    "\n",
    "assert gpu is not None\n",
    "torch.cuda.set_device(gpu)\n",
    "model = model.cuda(gpu)\n",
    "\n",
    "trainable_param = model.prompt_learner.parameters()\n",
    "optimizer = torch.optim.AdamW(trainable_param, lr)\n",
    "optim_state = deepcopy(optimizer.state_dict())\n",
    "\n",
    "# setup automatic mixed-precision (Amp) loss scaling\n",
    "scaler = torch.cuda.amp.GradScaler(init_scale=1000)\n",
    "\n",
    "print('=> Using native Torch AMP. Training in mixed precision.')\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating: Caltech101\n",
      "number of test samples: 2465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [01:06, 20.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 999/2465]\tTime  0.033 ( 0.067)\tAcc@1   0.00 ( 95.70)\tAcc@2 100.00 ( 98.60)\tAcc@3 100.00 ( 98.90)\tAcc@4 100.00 ( 99.40)\tAcc@5 100.00 ( 99.80)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2005it [02:06, 18.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [1999/2465]\tTime  0.064 ( 0.063)\tAcc@1 100.00 ( 92.65)\tAcc@2 100.00 ( 98.55)\tAcc@3 100.00 ( 99.15)\tAcc@4 100.00 ( 99.45)\tAcc@5 100.00 ( 99.75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2465it [02:33, 16.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *  Acc@1 92.941 Acc@2 98.499 Acc@3 99.270 Acc@4 99.513 Acc@5 99.757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1000it [05:43,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 999/2465]\tTime  0.285 ( 0.344)\tAcc@1   0.00 ( 83.70)\tAcc@2 100.00 ( 89.80)\tAcc@3 100.00 ( 92.00)\tAcc@4 100.00 ( 93.40)\tAcc@5 100.00 ( 94.20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [11:37,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [1999/2465]\tTime  0.289 ( 0.349)\tAcc@1 100.00 ( 75.40)\tAcc@2 100.00 ( 84.95)\tAcc@3 100.00 ( 87.60)\tAcc@4 100.00 ( 88.75)\tAcc@5 100.00 ( 89.55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2465it [14:02,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty caption count = 0\n",
      " *  Acc@1 75.091 Acc@2 84.381 Acc@3 86.775 Acc@4 87.951 Acc@5 88.641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "resolution = 224\n",
    "workers = 4\n",
    "dataset_mode = 'test'\n",
    "data = '/data/seongha'\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "    # norm stats from clip.load()\n",
    "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                     std=[0.26862954, 0.26130258, 0.27577711])\n",
    "    \n",
    "    # iterating through eval datasets\n",
    "datasets = test_sets.split(\"/\")\n",
    "results = {}\n",
    "\n",
    "# with open('inferece_image_caption{}.txt'.format(test_sets), 'w') as f:\n",
    "    # sys.stdout = f\n",
    "\n",
    "\n",
    "for set_id in datasets:\n",
    "    Dict = defaultdict(list)\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize(resolution, interpolation=BICUBIC),\n",
    "        transforms.CenterCrop(resolution),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    batchsize = 1\n",
    "    print(\"evaluating: {}\".format(set_id))\n",
    "    classnames = eval(\"{}_classes\".format(set_id.lower()))\n",
    "    model.reset_classnames(classnames, arch)\n",
    "\n",
    "    val_dataset = build_dataset(set_id, data_transform, data, mode=dataset_mode)\n",
    "    total_length = len(val_dataset)\n",
    "    print(\"number of test samples: {}\".format(len(val_dataset)))\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=batchsize, shuffle=False,\n",
    "                num_workers=workers, pin_memory=True)\n",
    "        \n",
    "    results['image'], tmp = test_time_adapt_eval_image(val_loader, model, model_state, optimizer, optim_state, scaler, Dict)\n",
    "    Dict = tmp\n",
    "    results['caption'], tmp = test_time_adapt_eval_caption(val_loader, model, model_state, optimizer, optim_state, scaler, Dict)\n",
    "    Dict = tmp\n",
    "    # assert len(Dict['image_path']) == len(Dict['caption']) and len(Dict['image_correct']) == len(Dict['caption_correct']), [len(v) for k, v in Dict.items()]\n",
    "    del val_dataset, val_loader\n",
    "\n",
    "    # try:\n",
    "    #     print(\"=> Acc. on testset [{}]: @1 {}/ @2 {}/ @3 {}/ @4 {}/ @5 {}\".format(set_id, results[set_id][0], results[set_id][1], results['image'][2], results['image'][3], results['image'[4], results['image'][5]]))\n",
    "    # except:\n",
    "    #     print(\"=> Acc. on testset [{}]: {}\".format(set_id, results[set_id]))\n",
    "# sys.stdout = sys.__stdout__\n",
    "    tmp = {k: v for k, v in Dict.items() if \"image\" in k}\n",
    "    df_img = pd.DataFrame(tmp)\n",
    "    df_img = df_img.reset_index()\n",
    "\n",
    "    tmp = {k: v for k, v in Dict.items() if \"caption\" in k}\n",
    "    df_cap = pd.DataFrame(tmp)\n",
    "    df_cap = df_cap.reset_index()\n",
    "\n",
    "    path = './notebook/inference_image_caption'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    # df.to_csv(os.path.join(path, 'inference_image_caption_{}.csv'.format(set_id)))\n",
    "\n",
    "    with open(os.path.join(path,'inference_image_caption_{}.txt'.format(set_id)), 'w') as f:\n",
    "        \n",
    "        img_corr_ind = df_img.loc[df_img['image_correct'] == 1, 'index'].to_list()\n",
    "        cap_corr_ind = df_cap.loc[df_cap['caption_correct'] == 1, 'index'].to_list()\n",
    "        neither = set(df_img.loc[df_img['image_correct'] == 0, 'index'].to_list()) & set(df_cap.loc[df_cap['caption_correct'] == 0, 'index'].to_list())\n",
    "        #image accuracy, caption ensemble accuracy\n",
    "        f.write(\"1. Image accuracy {:.4f}, Caption Accuracy {:.4f}\\n\".format( len(img_corr_ind)/total_length, len(cap_corr_ind)/total_length))\n",
    "        # 4 cases\n",
    "        union = set(img_corr_ind) | set(cap_corr_ind)\n",
    "        f.write(\"2: 4 cases\\n\")\n",
    "        f.write(\"Union :{}\\n\".format( len(union)))\n",
    "        intersection = set(img_corr_ind) & set(cap_corr_ind)\n",
    "        f.write(\"intersection {}\\n\".format( len(intersection)))\n",
    "        img_diff = set(img_corr_ind) - set(cap_corr_ind)\n",
    "        f.write(\"image only {}\\n\".format(len(img_diff)))\n",
    "        cap_diff = set(cap_corr_ind) - set(img_corr_ind)\n",
    "        f.write(\"cap only {}\\n\".format( len(cap_diff)))\n",
    "        f.write(\"neither {}\\n\".format( len(neither)))\n",
    "        # max accuracy\n",
    "        f.write(\"3. Max accuracy\\n\")\n",
    "        f.write(\"Max accuracy: {:.4f}\\n\".format(len(union)/total_length*100)) \n",
    "        #entropy, logit gap\n",
    "        f.write(\"4. Entropy & Logit Gap\\n\")\n",
    "        f.write(\"Image\\n\")\n",
    "        img_correct = df_img.loc[df_img['image_correct'] == 1]\n",
    "        f.write(\"correct\\n\")\n",
    "        f.write(\" {}\\n\".format(str(img_correct.shape)))\n",
    "        f.write(\"top1 - top2 mean, std\\n\")\n",
    "        f.write(\"{} {}\\n\".format(img_correct['image_gap'].astype(float).mean(), img_correct['image_gap'].astype(float).std() ))\n",
    "        f.write(\"Entropy mean {}\\n\".format(str(img_correct['image_entropy'].astype(float).mean())))\n",
    "        f.write(\"\")\n",
    "        img_wrong = df_img.loc[df_img['image_correct'] == 0]\n",
    "        f.write(\"wrong\\n\")\n",
    "        f.write(\" {}\\n\".format(str(img_wrong.shape)))\n",
    "        f.write(\"pred(top1) - target mean, std\\n\")\n",
    "        f.write(\"{} {}\\n\".format(img_wrong['image_gap'].astype(float).mean(), img_wrong['image_gap'].astype(float).std() ))\n",
    "        f.write(\"Entropy mean {}\\n\".format(str(img_wrong['image_entropy'].astype(float).mean())))\n",
    "        f.write(\"-\"*10 + \"\\n\")\n",
    "        \n",
    "        f.write(\"Caption\\n\")\n",
    "        cap_correct = df_cap.loc[df_cap['caption_correct'] == 1]\n",
    "        f.write(\"correct\\n\")\n",
    "        f.write(\" {}\\n\".format(str(cap_correct.shape)))\n",
    "        f.write(\"top1 - top2 mean, std\\n\")\n",
    "        f.write(\"{} {}\\n\".format(cap_correct['caption_gap'].astype(float).mean(), cap_correct['caption_gap'].astype(float).std() ))\n",
    "        f.write(\"Entropy mean {}\\n\".format(str(cap_correct['caption_entropy'].astype(float).mean())))\n",
    "        f.write(\"\")\n",
    "        cap_wrong = df_cap.loc[df_cap['caption_correct'] == 0]\n",
    "        f.write(\"wrong\\n\")\n",
    "        f.write(\" {}\\n\".format(str(cap_wrong.shape)))\n",
    "        f.write(\"pred(top1) - target mean, std\\n\")\n",
    "        f.write(\"{} {}\\n\".format(cap_wrong['caption_gap'].astype(float).mean(), cap_wrong['caption_gap'].astype(float).std() ))\n",
    "        f.write(\"Entropy mean {}\\n\".format(str(cap_wrong['caption_entropy'].astype(float).mean())))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_diff (508, 10)\n",
      "Image Entropy mean 0.6352336614173227\n",
      "Caption Entropy mean 1.1352950787401574\n",
      "\n",
      "cap_diff (70, 10)\n",
      "Image Entropy mean 1.0092857142857141\n",
      "Caption Entropy mean 0.3035428571428571\n",
      "\n",
      "intersection (1782, 10)\n",
      "Image Entropy mean 0.3845969135802469\n",
      "Caption Entropy mean 0.18090819304152636\n",
      "\n",
      "neither (105, 10)\n",
      "Image Entropy mean 1.1489142857142858\n",
      "Caption Entropy mean 0.7517161904761906\n",
      "\n",
      "union (2360, 10)\n",
      "Image Entropy mean 0.4570764406779661\n",
      "Caption Entropy mean 0.38998148305084746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(Dict)\n",
    "for i_set, i_name in zip([img_diff, cap_diff, intersection, neither, union], ['img_diff', 'cap_diff', 'intersection', 'neither', 'union']):\n",
    "    df_ = df.iloc[list(i_set)]\n",
    "    print(i_name, df_.shape)\n",
    "    print(\"Image Entropy mean {}\".format(str(df_['image_entropy'].astype(float).mean())))\n",
    "    print(\"Caption Entropy mean {}\\n\".format(str(df_['caption_entropy'].astype(float).mean())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
