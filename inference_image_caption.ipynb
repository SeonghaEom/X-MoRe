{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "\n",
    "import time\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "import torchvision.models as models\n",
    "\n",
    "import clip\n",
    "from clip.custom_clip import get_coop\n",
    "from clip.cocoop import get_cocoop\n",
    "from data.imagnet_prompts import imagenet_classes\n",
    "from data.datautils import AugMixAugmenter, build_dataset\n",
    "from utils.tools import Summary, AverageMeter, ProgressMeter, load_model_weight, set_random_seed, create_logger\n",
    "from data.cls_to_names import *\n",
    "from data.fewshot_datasets import fewshot_datasets\n",
    "from data.imagenet_variants import thousand_k_to_200, imagenet_a_mask, imagenet_r_mask, imagenet_v_mask\n",
    "from clip_retrieval.clip_client import ClipClient, Modality\n",
    "\n",
    "from collections import defaultdict\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "client = ClipClient(\n",
    "    url=\"http://127.0.0.1:1234/knn-service\",\n",
    "    indice_name='laion_400m',\n",
    "    modality=Modality.IMAGE,\n",
    "    num_images=10,\n",
    "    deduplicate=False,\n",
    ")\n",
    "client_backup = ClipClient(\n",
    "    url=\"http://127.0.0.1:1234/knn-service\",\n",
    "    indice_name='laion_400m',\n",
    "    modality=Modality.IMAGE,\n",
    "    num_images=200,\n",
    "    deduplicate=False,\n",
    ")\n",
    "\n",
    "client_backup2 = ClipClient(\n",
    "    url=\"http://127.0.0.1:1234/knn-service\",\n",
    "    indice_name='laion_400m',\n",
    "    modality=Modality.IMAGE,\n",
    "    num_images=1000,\n",
    "    deduplicate=False,\n",
    ")\n",
    "\n",
    "## Class to names mapping\n",
    "fewshot_datasets = ['DTD', 'Flower102', 'Food101', 'Cars', 'SUN397', \n",
    "                    'Aircraft', 'Pets', 'Caltech101', 'UCF101', 'eurosat']\n",
    "test_sets = 'Caltech101'\n",
    "if test_sets == 'Caltech101':\n",
    "    cls2names = ['face', 'leopard', 'motorbike', 'accordion', 'airplane', 'anchor', 'ant', 'barrel', 'bass', 'beaver', 'binocular', 'bonsai', 'brain', 'brontosaurus', 'buddha', 'butterfly', 'camera', 'cannon', 'car_side', 'ceiling_fan', 'cellphone', 'chair', 'chandelier', 'cougar_body', 'cougar_face', 'crab', 'crayfish', 'crocodile', 'crocodile_head', 'cup', 'dalmatian', 'dollar_bill', 'dolphin', 'dragonfly', 'electric_guitar', 'elephant', 'emu', 'euphonium', 'ewer', 'ferry', 'flamingo', 'flamingo_head', 'garfield', 'gerenuk', 'gramophone', 'grand_piano', 'hawksbill', 'headphone', 'hedgehog', 'helicopter', 'ibis', 'inline_skate', 'joshua_tree', 'kangaroo', 'ketch', 'lamp', 'laptop', 'llama', 'lobster', 'lotus', 'mandolin', 'mayfly', 'menorah', 'metronome', 'minaret', 'nautilus', 'octopus', 'okapi', 'pagoda', 'panda', 'pigeon', 'pizza', 'platypus', 'pyramid', 'revolver', 'rhino', 'rooster', 'saxophone', 'schooner', 'scissors', 'scorpion', 'sea_horse', 'snoopy', 'soccer_ball', 'stapler', 'starfish', 'stegosaurus', 'stop_sign', 'strawberry', 'sunflower', 'tick', 'trilobite', 'umbrella', 'watch', 'water_lilly', 'wheelchair', 'wild_cat', 'windsor_chair', 'wrench', 'yin_yang']\n",
    "elif test_sets == 'DTD':\n",
    "    cls2names = dtd_classes\n",
    "elif test_sets =='Cars':\n",
    "    cls2names = cars_classes\n",
    "    # print(cls2names[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_confident_samples(logits, top):\n",
    "    batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "    idx = torch.argsort(batch_entropy, descending=False)[:int(batch_entropy.size()[0] * top)]\n",
    "    return logits[idx], idx\n",
    "\n",
    "def avg_entropy(outputs):\n",
    "    # epsilon = 1e-10\n",
    "    assert len(outputs) > 0\n",
    "    assert torch.any(torch.isnan(outputs)) == False\n",
    "    logits = outputs - outputs.logsumexp(dim=-1, keepdim=True) # logits = outputs.log_softmax(dim=1) [N, 1000]\n",
    "    assert torch.any(torch.isnan(logits)) == False\n",
    "    avg_logits = logits.logsumexp(dim=0) - np.log(logits.shape[0]) # avg_logits = logits.mean(0) [1, 1000]\n",
    "    # print(avg_logits)\n",
    "    if torch.any(torch.isnan(avg_logits)):\n",
    "        print(\"average logits \", outputs.log_softmax(dim=1).mean(0))\n",
    "    assert torch.any(torch.isnan(avg_logits)) == False\n",
    "    \n",
    "    min_real = torch.finfo(avg_logits.dtype).min\n",
    "    avg_logits = torch.clamp(avg_logits, min=min_real)\n",
    "    assert torch.any(torch.isnan(avg_logits)) == False\n",
    "    return -((avg_logits) * (torch.exp(avg_logits))).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy(output, target, topk=(1,), caption=None, logger=None):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "        if output.shape[0] == 1:#only image prediction\n",
    "            logit_k, pred = output.topk(maxk, 1, True, True)\n",
    "            pred = pred.t()\n",
    "            correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        else: # evaluate captions\n",
    "            bag = []\n",
    "            # # length = max(5, output.shape[0]-1)\n",
    "            # cap_pred = output[1:]\n",
    "            # # cap_pred = torch.mean(cap_pred, 0,  keepdim=True)\n",
    "            # _, pred = cap_pred.topk(maxk, 1, True, True) #5, 1 #candidate labels\n",
    "            # pred = pred.reshape(maxk, 1)\n",
    "            pred = torch.mean(output, 0, keepdim=True)\n",
    "            _, pred = pred.topk(maxk, 1, True, True)\n",
    "            pred = pred.reshape(maxk, 1)\n",
    "            correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            if k == 1 and correct_k.item() == 0:\n",
    "\n",
    "                pred = pred.squeeze().tolist()\n",
    "                pred = [cls2names[lb] for lb in pred]\n",
    "\n",
    "                if logger: logger.info(\"wrong prediction, target {} & predicted value {}\".format(target, pred))\n",
    "            elif k==1 and correct_k.item() == 1:\n",
    "                # logger.info(\"wrong prediction , logit: \", output)\n",
    "                pred = pred.squeeze().tolist()\n",
    "                pred = [cls2names[lb] for lb in pred]\n",
    "\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "tta_steps = 1\n",
    "which_loss = \"cosine\"\n",
    "gpu = 3\n",
    "print_freq = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_caption(img_path, retrieve_K=1):\n",
    "    try:\n",
    "        query_res = client.query(image=img_path)[:retrieve_K]\n",
    "        retrieved_txt= [D['caption'] for D in query_res]\n",
    "        retrieved_url = [D['url'] for D in query_res]\n",
    "        retrieved_score = [D['similarity'] for D in query_res]\n",
    "    except:\n",
    "        # print(client.query(image=img_path))\n",
    "        query_res = client_backup.query(image=img_path)[:retrieve_K]\n",
    "        retrieved_txt= [D['caption'] for D in query_res]\n",
    "        retrieved_url = [D['url'] for D in query_res]\n",
    "        retrieved_score = [D['similarity'] for D in query_res]\n",
    "    if len(retrieved_txt) != retrieve_K:\n",
    "        query_res = client_backup2.query(image=img_path)[:retrieve_K]\n",
    "        retrieved_txt= [D['caption'] for D in query_res]\n",
    "        retrieved_url = [D['url'] for D in query_res]\n",
    "        retrieved_score = [D['similarity'] for D in query_res]\n",
    "        # output = model.caption_ensemble(retrieved_txt)\n",
    "        \n",
    "    return retrieved_txt\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time_adapt_eval_image(val_loader, model, model_state, optimizer, optim_state, scaler, save_result=None):\n",
    "    batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f', Summary.AVERAGE)\n",
    "    top2 = AverageMeter('Acc@2', ':6.2f', Summary.AVERAGE)\n",
    "    top3 = AverageMeter('Acc@3', ':6.2f', Summary.AVERAGE)\n",
    "    top4 = AverageMeter('Acc@4', ':6.2f', Summary.AVERAGE)\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f', Summary.AVERAGE)\n",
    "    \n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, top1, top2, top3, top4, top5],\n",
    "        prefix='Test: ',\n",
    "        logger = None)\n",
    "\n",
    "    # reset model and switch to evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.reset()\n",
    "    end = time.time()\n",
    "    cnt_empty = 0\n",
    "    if save_result == None: save_result = defaultdict(list)\n",
    "    \n",
    "    for i, (image, target, imagepath) in tqdm(enumerate(val_loader)): \n",
    "        assert gpu is not None\n",
    "        # print(\"Image Path \", imagepath[0])\n",
    "        save_result['image_path'].append(imagepath[0])\n",
    "        \n",
    "        target = target.cuda(gpu, non_blocking=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                #image\n",
    "                output, output_cap = model.inference(image.cuda(gpu, non_blocking=True), caption= None)\n",
    "                \n",
    "        logit_k, pred = output.topk(1, 1, True, True) #1,1\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        correct = correct.reshape(-1).float().sum(0, keepdim=True).item() #1 or 0\n",
    "        save_result['image_correct'].append(int(correct))\n",
    "                \n",
    "        acc1, acc2, acc3, acc4, acc5 = accuracy(output, target, topk=(1, 2, 3, 4, 5), caption=None, logger=None)\n",
    "        # print(acc1, acc2, acc3, acc4, acc5)\n",
    "        top1.update(acc1[0], image.size(0))\n",
    "        top2.update(acc2[0], image.size(0))\n",
    "        top3.update(acc3[0], image.size(0))\n",
    "        top4.update(acc4[0], image.size(0))\n",
    "        top5.update(acc5[0], image.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if (i+1) % print_freq == 0:\n",
    "            progress.display(i)\n",
    "        \n",
    "\n",
    "    progress.display_summary()\n",
    "    return [top1.avg, top2.avg, top3.avg, top4.avg, top5.avg], save_result\n",
    "\n",
    "def test_time_adapt_eval_caption(val_loader, model, model_state, optimizer, optim_state, scaler, save_result=False):\n",
    "    batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f', Summary.AVERAGE)\n",
    "    top2 = AverageMeter('Acc@2', ':6.2f', Summary.AVERAGE)\n",
    "    top3 = AverageMeter('Acc@3', ':6.2f', Summary.AVERAGE)\n",
    "    top4 = AverageMeter('Acc@4', ':6.2f', Summary.AVERAGE)\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f', Summary.AVERAGE)\n",
    "    \n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, top1, top2, top3, top4, top5],\n",
    "        prefix='Test: ',\n",
    "        logger = None)\n",
    "\n",
    "    # reset model and switch to evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.reset()\n",
    "    end = time.time()\n",
    "    cnt_empty = 0\n",
    "    assert save_result != None\n",
    "    \n",
    "    for i, (image, target, imagepath) in tqdm(enumerate(val_loader)): \n",
    "        assert gpu is not None\n",
    "        # print(\"Image Path \", imagepath[0])\n",
    "        # save_result['image_path'] = imagepath[0]\n",
    "        \n",
    "        target = target.cuda(gpu, non_blocking=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                #caption\n",
    "                retrieved_Caption = return_caption(imagepath[0])\n",
    "                save_result['caption'].append(retrieved_Caption[0])\n",
    "                output_caption = model.caption_ensemble(retrieved_Caption)\n",
    "\n",
    "        logit_k, pred = output_caption.topk(1, 1, True, True) #1,1\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        correct = correct.reshape(-1).float().sum(0, keepdim=True).item() #1 or 0\n",
    "        save_result['caption_correct'].append(int(correct))\n",
    "         \n",
    "        acc1, acc2, acc3, acc4, acc5 = accuracy(output_caption, target, topk=(1, 2, 3, 4, 5), caption=None, logger=None)\n",
    "        # print(acc1, acc2, acc3, acc4, acc5)\n",
    "        top1.update(acc1[0], image.size(0))\n",
    "        top2.update(acc2[0], image.size(0))\n",
    "        top3.update(acc3[0], image.size(0))\n",
    "        top4.update(acc4[0], image.size(0))\n",
    "        top5.update(acc5[0], image.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if (i+1) % print_freq == 0:\n",
    "            progress.display(i)\n",
    "    \n",
    "\n",
    "    progress.display_summary()\n",
    "    return [top1.avg, top2.avg, top3.avg, top4.avg, top5.avg], save_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##parameters\n",
    "fewshot_datasets = ['DTD', 'Flower102', 'Food101', 'Cars', 'SUN397', \n",
    "                    'Aircraft', 'Pets', 'Caltech101', 'UCF101', 'eurosat']\n",
    "arch='ViT-B/16'\n",
    "n_ctx=4\n",
    "ctx_init=\"a_photo_of_a\"\n",
    "lr = 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the contect with given words: [a_photo_of_a]\n",
      "Initial context: \"a photo of a\"\n",
      "Number of context words (tokens): 4\n",
      "tuing parameters  {'prompt_learner.ctx'}\n",
      "=> Model created: visual backbone ViT-B/16\n",
      "=> Using native Torch AMP. Training in mixed precision.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "if test_sets in fewshot_datasets:\n",
    "    classnames = eval(\"{}_classes\".format(test_sets.lower()))\n",
    "model = get_coop(arch, test_sets, gpu, n_ctx, ctx_init)\n",
    "model_state = None\n",
    "\n",
    "cross_check = set()\n",
    "for name, param in model.named_parameters():\n",
    "\n",
    "    if \"prompt_learner\" not in name:\n",
    "        param.requires_grad_(False)\n",
    "    if param.requires_grad : cross_check.add(name)\n",
    "print(\"tuing parameters \", cross_check)\n",
    "\n",
    "print(\"=> Model created: visual backbone {}\".format(arch))\n",
    "\n",
    "assert gpu is not None\n",
    "torch.cuda.set_device(gpu)\n",
    "model = model.cuda(gpu)\n",
    "\n",
    "trainable_param = model.prompt_learner.parameters()\n",
    "optimizer = torch.optim.AdamW(trainable_param, lr)\n",
    "optim_state = deepcopy(optimizer.state_dict())\n",
    "\n",
    "# setup automatic mixed-precision (Amp) loss scaling\n",
    "scaler = torch.cuda.amp.GradScaler(init_scale=1000)\n",
    "\n",
    "print('=> Using native Torch AMP. Training in mixed precision.')\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating: Caltech101\n",
      "number of test samples: 2465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "206it [00:06, 32.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 199/2465]\tTime  0.031 ( 0.032)\tAcc@1 100.00 ( 95.00)\tAcc@2 100.00 ( 98.50)\tAcc@3 100.00 ( 99.00)\tAcc@4 100.00 ( 99.50)\tAcc@5 100.00 (100.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "406it [00:12, 32.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 399/2465]\tTime  0.030 ( 0.032)\tAcc@1 100.00 ( 97.50)\tAcc@2 100.00 ( 99.25)\tAcc@3 100.00 ( 99.50)\tAcc@4 100.00 ( 99.75)\tAcc@5 100.00 (100.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "606it [00:18, 32.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 599/2465]\tTime  0.030 ( 0.031)\tAcc@1 100.00 ( 98.00)\tAcc@2 100.00 ( 99.33)\tAcc@3 100.00 ( 99.50)\tAcc@4 100.00 ( 99.83)\tAcc@5 100.00 (100.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "806it [00:24, 31.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 799/2465]\tTime  0.035 ( 0.031)\tAcc@1 100.00 ( 96.25)\tAcc@2 100.00 ( 98.50)\tAcc@3 100.00 ( 98.75)\tAcc@4 100.00 ( 99.25)\tAcc@5 100.00 ( 99.75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1006it [00:31, 31.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 999/2465]\tTime  0.030 ( 0.031)\tAcc@1   0.00 ( 95.70)\tAcc@2 100.00 ( 98.60)\tAcc@3 100.00 ( 98.90)\tAcc@4 100.00 ( 99.40)\tAcc@5 100.00 ( 99.80)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1206it [00:37, 31.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [1199/2465]\tTime  0.032 ( 0.031)\tAcc@1 100.00 ( 94.00)\tAcc@2 100.00 ( 98.67)\tAcc@3 100.00 ( 99.00)\tAcc@4 100.00 ( 99.42)\tAcc@5 100.00 ( 99.83)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1406it [00:43, 31.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [1399/2465]\tTime  0.030 ( 0.031)\tAcc@1 100.00 ( 93.00)\tAcc@2 100.00 ( 98.71)\tAcc@3 100.00 ( 99.14)\tAcc@4 100.00 ( 99.50)\tAcc@5 100.00 ( 99.86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1606it [00:50, 31.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [1599/2465]\tTime  0.030 ( 0.031)\tAcc@1 100.00 ( 93.81)\tAcc@2 100.00 ( 98.81)\tAcc@3 100.00 ( 99.19)\tAcc@4 100.00 ( 99.56)\tAcc@5 100.00 ( 99.88)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1806it [00:56, 31.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [1799/2465]\tTime  0.030 ( 0.031)\tAcc@1 100.00 ( 92.00)\tAcc@2 100.00 ( 98.44)\tAcc@3 100.00 ( 99.11)\tAcc@4 100.00 ( 99.44)\tAcc@5 100.00 ( 99.78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2006it [01:02, 31.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [1999/2465]\tTime  0.031 ( 0.031)\tAcc@1 100.00 ( 92.65)\tAcc@2 100.00 ( 98.55)\tAcc@3 100.00 ( 99.15)\tAcc@4 100.00 ( 99.45)\tAcc@5 100.00 ( 99.75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2206it [01:09, 31.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [2199/2465]\tTime  0.031 ( 0.031)\tAcc@1 100.00 ( 92.50)\tAcc@2 100.00 ( 98.45)\tAcc@3 100.00 ( 99.18)\tAcc@4 100.00 ( 99.45)\tAcc@5 100.00 ( 99.73)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2406it [01:15, 31.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [2399/2465]\tTime  0.030 ( 0.031)\tAcc@1 100.00 ( 93.04)\tAcc@2 100.00 ( 98.58)\tAcc@3 100.00 ( 99.25)\tAcc@4 100.00 ( 99.50)\tAcc@5 100.00 ( 99.75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2465it [01:17, 31.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *  Acc@1 92.941 Acc@2 98.499 Acc@3 99.270 Acc@4 99.513 Acc@5 99.757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "200it [00:49,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 199/2465]\tTime  0.241 ( 0.246)\tAcc@1 100.00 ( 37.50)\tAcc@2 100.00 ( 50.50)\tAcc@3 100.00 ( 73.50)\tAcc@4 100.00 ( 75.50)\tAcc@5 100.00 ( 76.50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [01:36,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 399/2465]\tTime  0.226 ( 0.242)\tAcc@1 100.00 ( 53.00)\tAcc@2 100.00 ( 63.75)\tAcc@3 100.00 ( 75.75)\tAcc@4 100.00 ( 76.75)\tAcc@5 100.00 ( 77.25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [02:24,  4.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 599/2465]\tTime  0.232 ( 0.241)\tAcc@1 100.00 ( 53.50)\tAcc@2 100.00 ( 62.67)\tAcc@3 100.00 ( 71.83)\tAcc@4 100.00 ( 74.50)\tAcc@5 100.00 ( 75.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [03:11,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 799/2465]\tTime  0.217 ( 0.240)\tAcc@1   0.00 ( 46.88)\tAcc@2   0.00 ( 54.88)\tAcc@3   0.00 ( 62.50)\tAcc@4   0.00 ( 66.25)\tAcc@5   0.00 ( 67.75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [03:59,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 999/2465]\tTime  0.261 ( 0.240)\tAcc@1 100.00 ( 46.10)\tAcc@2 100.00 ( 54.30)\tAcc@3 100.00 ( 61.20)\tAcc@4 100.00 ( 64.70)\tAcc@5 100.00 ( 66.90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1200it [04:47,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [1199/2465]\tTime  0.246 ( 0.240)\tAcc@1 100.00 ( 41.67)\tAcc@2 100.00 ( 50.00)\tAcc@3 100.00 ( 56.75)\tAcc@4 100.00 ( 59.92)\tAcc@5 100.00 ( 62.33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1400it [05:34,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [1399/2465]\tTime  0.236 ( 0.239)\tAcc@1   0.00 ( 38.00)\tAcc@2   0.00 ( 46.07)\tAcc@3   0.00 ( 52.93)\tAcc@4   0.00 ( 55.86)\tAcc@5   0.00 ( 58.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1600it [06:22,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [1599/2465]\tTime  0.251 ( 0.239)\tAcc@1   0.00 ( 37.25)\tAcc@2   0.00 ( 44.56)\tAcc@3   0.00 ( 51.25)\tAcc@4   0.00 ( 54.12)\tAcc@5   0.00 ( 56.19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1800it [07:09,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [1799/2465]\tTime  0.211 ( 0.239)\tAcc@1   0.00 ( 35.00)\tAcc@2   0.00 ( 42.11)\tAcc@3   0.00 ( 48.78)\tAcc@4   0.00 ( 51.94)\tAcc@5   0.00 ( 54.11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [07:57,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [1999/2465]\tTime  0.276 ( 0.239)\tAcc@1 100.00 ( 33.10)\tAcc@2 100.00 ( 40.05)\tAcc@3 100.00 ( 46.25)\tAcc@4 100.00 ( 49.15)\tAcc@5 100.00 ( 51.20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2200it [08:44,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [2199/2465]\tTime  0.208 ( 0.239)\tAcc@1   0.00 ( 31.36)\tAcc@2   0.00 ( 37.91)\tAcc@3   0.00 ( 43.95)\tAcc@4   0.00 ( 47.00)\tAcc@5   0.00 ( 49.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2400it [09:32,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [2399/2465]\tTime  0.264 ( 0.239)\tAcc@1   0.00 ( 32.88)\tAcc@2   0.00 ( 39.08)\tAcc@3 100.00 ( 44.83)\tAcc@4 100.00 ( 47.67)\tAcc@5 100.00 ( 49.62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2465it [09:48,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *  Acc@1 32.414 Acc@2 38.621 Acc@3 44.300 Acc@4 47.099 Acc@5 49.087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Caltech101'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 47\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m=> Acc. on testset [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m]: @1 \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/ @2 \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/ @3 \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/ @4 \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/ @5 \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(set_id, results[set_id][\u001b[39m0\u001b[39m], results[set_id][\u001b[39m1\u001b[39m], results[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m2\u001b[39m], results[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m3\u001b[39m], results[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m[\u001b[39m4\u001b[39m], results[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m5\u001b[39m]]))\n\u001b[1;32m     48\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Caltech101'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m=> Acc. on testset [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m]: @1 \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/ @2 \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/ @3 \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/ @4 \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/ @5 \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(set_id, results[set_id][\u001b[39m0\u001b[39m], results[set_id][\u001b[39m1\u001b[39m], results[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m2\u001b[39m], results[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m3\u001b[39m], results[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m[\u001b[39m4\u001b[39m], results[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m5\u001b[39m]]))\n\u001b[1;32m     48\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m=> Acc. on testset [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m]: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(set_id, results[set_id]))\n\u001b[1;32m     50\u001b[0m \u001b[39m# sys.stdout = sys.__stdout__\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Caltech101'"
     ]
    }
   ],
   "source": [
    "resolution = 224\n",
    "workers = 4\n",
    "dataset_mode = 'test'\n",
    "data = '/data/seongha'\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "    # norm stats from clip.load()\n",
    "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                     std=[0.26862954, 0.26130258, 0.27577711])\n",
    "    \n",
    "    # iterating through eval datasets\n",
    "datasets = test_sets.split(\"/\")\n",
    "results = {}\n",
    "\n",
    "# with open('inferece_image_caption{}.txt'.format(test_sets), 'w') as f:\n",
    "    # sys.stdout = f\n",
    "Dict = defaultdict(list)\n",
    "for set_id in datasets:\n",
    "\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize(resolution, interpolation=BICUBIC),\n",
    "        transforms.CenterCrop(resolution),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    batchsize = 1\n",
    "    print(\"evaluating: {}\".format(set_id))\n",
    "    classnames = eval(\"{}_classes\".format(set_id.lower()))\n",
    "    model.reset_classnames(classnames, arch)\n",
    "\n",
    "    val_dataset = build_dataset(set_id, data_transform, data, mode=dataset_mode)\n",
    "    print(\"number of test samples: {}\".format(len(val_dataset)))\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=batchsize, shuffle=False,\n",
    "                num_workers=workers, pin_memory=True)\n",
    "        \n",
    "    results['image'], tmp = test_time_adapt_eval_image(val_loader, model, model_state, optimizer, optim_state, scaler, Dict)\n",
    "    Dict = tmp\n",
    "    results['caption'], tmp = test_time_adapt_eval_caption(val_loader, model, model_state, optimizer, optim_state, scaler, Dict)\n",
    "    Dict = tmp\n",
    "    assert len(Dict['image_path']) == len(Dict['caption']) and len(Dict['image_correct']) == len(Dict['caption_correct']), [len(v) for k, v in Dict.items()]\n",
    "    del val_dataset, val_loader\n",
    "\n",
    "    # try:\n",
    "    #     print(\"=> Acc. on testset [{}]: @1 {}/ @2 {}/ @3 {}/ @4 {}/ @5 {}\".format(set_id, results[set_id][0], results[set_id][1], results['image'][2], results['image'][3], results['image'[4], results['image'][5]]))\n",
    "    # except:\n",
    "    #     print(\"=> Acc. on testset [{}]: {}\".format(set_id, results[set_id]))\n",
    "# sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'image_path', 'image_correct', 'caption', 'caption_correct'], dtype='object')\n",
      "   index                                         image_path  image_correct  \\\n",
      "0      0  /data/seongha/caltech-101/101_ObjectCategories...              1   \n",
      "1      1  /data/seongha/caltech-101/101_ObjectCategories...              1   \n",
      "\n",
      "                                             caption  caption_correct  \n",
      "0                   Female hand on white background.                0  \n",
      "1  Closeup portrait of adorable baby Royalty Free...                1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(Dict)\n",
    "df = df.reset_index()\n",
    "print(df.columns)\n",
    "print(df.head(2))\n",
    "\n",
    "img_corr_ind = df.loc[df['image_correct'] == 1, 'index'].to_list()\n",
    "cap_corr_ind = df.loc[df['caption_correct'] == 1, 'index'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2291 801\n",
      "1511\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "print(len(img_corr_ind), len(cap_corr_ind))\n",
    "print(len(set(img_corr_ind) - set(cap_corr_ind)))\n",
    "print(len(set(cap_corr_ind) - set(img_corr_ind)))\n",
    "\n",
    "union = set(img_corr_ind) | set(cap_corr_ind)\n",
    "intersection = set(img_corr_ind) & set(cap_corr_ind)\n",
    "img_diff = set(img_corr_ind) - set(cap_corr_ind)\n",
    "cap_diff = set(cap_corr_ind) - set(img_corr_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max accuracy: 93.7931\n"
     ]
    }
   ],
   "source": [
    "df.iloc[list(union)] \n",
    "print(\"Max accuracy: {:.4f}\".format(len(union)/2465*100)) #image accuracy = 92.941"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each, name in zip([union, intersection, img_diff, cap_diff],['union', 'intersection', 'img_diff', 'cap_diff']):\n",
    "    df.iloc[list(each)].to_csv('inference_correct_by_{}.csv'.format(name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
