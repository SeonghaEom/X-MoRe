{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seongha/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "import time\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "import torchvision.models as models\n",
    "\n",
    "import clip\n",
    "from clip.custom_clip import get_coop\n",
    "from clip.cocoop import get_cocoop\n",
    "from data.imagnet_prompts import imagenet_classes\n",
    "from data.datautils import AugMixAugmenter, build_dataset\n",
    "from utils.tools import Summary, AverageMeter, ProgressMeter, load_model_weight, set_random_seed, create_logger\n",
    "from data.cls_to_names import *\n",
    "from data.fewshot_datasets import fewshot_datasets\n",
    "from data.imagenet_variants import thousand_k_to_200, imagenet_a_mask, imagenet_r_mask, imagenet_v_mask\n",
    "from clip_retrieval.clip_client import ClipClient, Modality\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "client = ClipClient(\n",
    "    url=\"http://127.0.0.1:1234/knn-service\",\n",
    "    indice_name='laion_400m',\n",
    "    modality=Modality.IMAGE,\n",
    "    num_images=1000,\n",
    "    deduplicate=False,\n",
    ")\n",
    "client_backup = ClipClient(\n",
    "    url=\"http://127.0.0.1:1234/knn-service\",\n",
    "    indice_name='laion_400m',\n",
    "    modality=Modality.IMAGE,\n",
    "    num_images=200,\n",
    "    deduplicate=False,\n",
    ")\n",
    "\n",
    "client_backup2 = ClipClient(\n",
    "    url=\"http://127.0.0.1:1234/knn-service\",\n",
    "    indice_name='laion_400m',\n",
    "    modality=Modality.IMAGE,\n",
    "    num_images=1000,\n",
    "    deduplicate=False,\n",
    ")\n",
    "\n",
    "## Class to names mapping\n",
    "fewshot_datasets = ['DTD', 'Flower102', 'Food101', 'Cars', 'SUN397', \n",
    "                    'Aircraft', 'Pets', 'Caltech101', 'UCF101', 'eurosat']\n",
    "test_sets = 'Pets'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_confident_samples(logits, top):\n",
    "    batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "    idx = torch.argsort(batch_entropy, descending=False)[:int(batch_entropy.size()[0] * top)]\n",
    "    return logits[idx], idx\n",
    "\n",
    "def avg_entropy(outputs):\n",
    "    # epsilon = 1e-10\n",
    "    assert len(outputs) > 0\n",
    "    assert torch.any(torch.isnan(outputs)) == False\n",
    "    # logits = outputs - outputs.logsumexp(dim=-1, keepdim=True) # logits = outputs.log_softmax(dim=1) [N, 1000]\n",
    "    logits = outputs.log_softmax(dim=-1) #[N, 1000]\n",
    "    assert torch.any(torch.isnan(logits)) == False\n",
    "    # avg_logits = logits.logsumexp(dim=0) - np.log(logits.shape[0]) # avg_logits = logits.mean(0) [1, 1000]\n",
    "    avg_logits = logits.mean(0) #[1, 1000]\n",
    "    # print(avg_logits)\n",
    "    if torch.any(torch.isnan(avg_logits)):\n",
    "        print(\"average logits \", outputs.log_softmax(dim=1).mean(0))\n",
    "    assert torch.any(torch.isnan(avg_logits)) == False\n",
    "    \n",
    "    min_real = torch.finfo(avg_logits.dtype).min\n",
    "    avg_logits = torch.clamp(avg_logits, min=min_real)\n",
    "    assert torch.any(torch.isnan(avg_logits)) == False\n",
    "    return -((avg_logits) * (torch.exp(avg_logits))).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy(output, target, topk=(1,), caption=None, logger=None, args=None):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "            \n",
    "        pred = torch.mean(output, 0, keepdim=True)\n",
    "        logit_k, pred = pred.topk(maxk, 1, True, True)\n",
    "        pred = pred.reshape(maxk, 1)\n",
    "        \n",
    "        #majority voting\n",
    "        # bag = []\n",
    "        # for i, each in enumerate(output):\n",
    "        #     if i==0: # image\n",
    "        #         # continue\n",
    "        #         each = each.unsqueeze(0) #1, 1000\n",
    "        #         _, image_pred = each.topk(maxk, 1, True, True) #1, 5\n",
    "        #         image_pred = image_pred.t() #5, 1\n",
    "        #     else: #caption\n",
    "        #         each = each.unsqueeze(0) #1, 1000\n",
    "        #         _, pred = each.topk(maxk, 1, True, True)\n",
    "        #         pred = pred.t() #5, 1\n",
    "        #         for elem in pred.tolist(): bag.append(elem[0])\n",
    "        # # # _, pred = output.topk(maxk, 1, True, True)\n",
    "        # # # pred = pred.t()\n",
    "        # c = Counter(bag)\n",
    "\n",
    "        # pred = c.most_common(maxk)\n",
    "\n",
    "        # print(\"caption prediction\" , pred)\n",
    "        # print(\"label \", target.view(1, -1).expand_as(pred))\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        # if k == 1 and correct_k.item() == 0:\n",
    "        #     # print(\"-------wrong prediction-------\")\n",
    "        #     # logger.info(\"wrong prediction , logit: \", output)\n",
    "        #     pred = pred.squeeze().tolist()\n",
    "        #     pred = [cls2names[lb] for lb in pred]\n",
    "        #     # print(\"target: [{}]\".format( cls2names[target.squeeze().item()]))\n",
    "        #     # print(\"predicted category & logit: {}\".format(list(zip(pred, logit_k.squeeze().tolist()))))\n",
    "        #     # print(\"logit \", logit_k)\n",
    "        #     if logger: logger.info(\"wrong prediction, target {} & predicted value {}\".format(target, pred))\n",
    "        #     # print(\"-------------------------------\")\n",
    "        # elif k==1 and correct_k.item() == 1:\n",
    "        #     # print(\"-------correct prediction-------\")\n",
    "        #     # logger.info(\"wrong prediction , logit: \", output)\n",
    "        #     pred = pred.squeeze().tolist()\n",
    "        #     pred = [cls2names[lb] for lb in pred]\n",
    "        #     # print(\"target: [{}]\".format( cls2names[target.squeeze().item()]))\n",
    "        #     # print(\"predicted category & logit: {}\".format(list(zip(pred, logit_k.squeeze().tolist()))))\n",
    "            # print(\"-------------------------------\")\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "tta_steps = 1\n",
    "which_loss = \"entropy\"\n",
    "gpu = 4\n",
    "print_freq = 400\n",
    "retrieve_K = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time_adapt_eval(val_loader, model, model_state, optimizer, optim_state, scaler, save_result=False):\n",
    "    batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)\n",
    "    top1 = AverageMeter('Top@1', ':6.2f', Summary.AVERAGE)\n",
    "    top2 = AverageMeter('Top@2', ':6.2f', Summary.AVERAGE)\n",
    "    top4 = AverageMeter('Top@4', ':6.2f', Summary.AVERAGE)\n",
    "    top8 = AverageMeter('Top@8', ':6.2f', Summary.AVERAGE)\n",
    "    top16 = AverageMeter('Top@16', ':6.2f', Summary.AVERAGE)\n",
    "    top32 = AverageMeter('Top@32', ':6.2f', Summary.AVERAGE)\n",
    "    top64 = AverageMeter('Top@64', ':6.2f', Summary.AVERAGE)\n",
    "    top128 = AverageMeter('Top@128', ':6.2f', Summary.AVERAGE)\n",
    "\n",
    "    \n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, top1, top2, top4, top8, top16, top32, top64, top128],\n",
    "        prefix='Test: ',\n",
    "        logger = None)\n",
    "\n",
    "    # reset model and switch to evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.reset()\n",
    "    end = time.time()\n",
    "    assert save_result != None\n",
    "    cnt_empty = 0\n",
    "    \n",
    "    for i, (image, target, imagepath) in tqdm(enumerate(val_loader)): \n",
    "        assert gpu is not None\n",
    "        # print(\" \")\n",
    "        # print(\"Image Path \", imagepath)\n",
    "        target = target.cuda(gpu, non_blocking=True)\n",
    "        \n",
    "        ### One time training\n",
    "        # reset the tunable prompt to its initial state\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.reset()\n",
    "        optimizer.load_state_dict(optim_state)\n",
    "                \n",
    "        # retrieved_caption = test_time_tuning(model, image.cuda(gpu, non_blocking=True), optimizer, scaler)\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                img_path = imagepath[0]\n",
    "                \n",
    "                try:\n",
    "                    query_res = client.query(image=img_path)[:retrieve_K]\n",
    "                    retrieved_txt= [D['caption'] for D in query_res]\n",
    "                    retrieved_url = [D['url'] for D in query_res]\n",
    "                    retrieved_score = [D['similarity'] for D in query_res]\n",
    "                    assert len(query_res) == retrieve_K\n",
    "                except:\n",
    "                    # print(client.query(image=img_path))\n",
    "                    query_res = client_backup2.query(image=img_path)[:retrieve_K]\n",
    "                    retrieved_txt= [D['caption'] for D in query_res]\n",
    "                    retrieved_url = [D['url'] for D in query_res]\n",
    "                    retrieved_score = [D['similarity'] for D in query_res]\n",
    "                if len(retrieved_txt) == retrieve_K:\n",
    "                    output = model.caption_ensemble(retrieved_txt)\n",
    "                    # output_merged = model.caption_ensemble(retrieved_txt, retrieved_score)\n",
    "                else:\n",
    "                    cnt_empty +=1\n",
    "                    continue\n",
    "\n",
    "        ent = avg_entropy(output)\n",
    "        save_result['caption_entropy'].append('{:.4f}'.format(ent))\n",
    "        save_result['image_path'].append(imagepath[0])\n",
    "        weighted =[]\n",
    "        assert len(retrieved_score) == len(output), (len(retrieved_score), len(output))\n",
    "        for score, logit in zip(retrieved_score, output):\n",
    "            logit = torch.nn.functional.softmax(logit, dim=-1)\n",
    "            weighted.append(score/sum(retrieved_score) * logit)\n",
    "\n",
    "        for c_n, topN_meter in zip([1, 2, 4, 8, 16, 32, 64, 128], [top1, top2, top4, top8, top16, top32, top64, top128]):\n",
    "            # print(torch.cat(tuple(weighted[:c_n]), axis=0).shape)\n",
    "            tmp = weighted[:c_n]\n",
    "            assert len(tmp) == c_n\n",
    "            # print(torch.cat(weighted[:c_n], dim=0).shape)\n",
    "            tmp = torch.sum(torch.cat(weighted[:c_n]).reshape(c_n, -1), axis=0).reshape(1, -1)\n",
    "            # print(tmp.shape)\n",
    "            acc1 = accuracy(tmp, target, topk=(1,), caption=None, logger=None)\n",
    "            # print(acc1[0])\n",
    "            topN_meter.update(acc1[0].item(), image.size(0))\n",
    "            \n",
    "        # # Logit Gap\n",
    "        # logit_k, pred = weighted.topk(2, 1, True, True)\n",
    "        # logit_k = logit_k.squeeze()\n",
    "        # pred = pred[:,0].t()\n",
    "        # correct = pred.eq(target)\n",
    "        # correct = correct.reshape(-1).float().sum(0, keepdim=True).item()\n",
    "        # save_result['caption_correct'].append(int(correct))\n",
    "        # save_result['caption_logit'].append('{:4f}'.format(logit_k[0].item()))\n",
    "        # if correct == 1:\n",
    "        #     save_result['caption_gap'].append('{:.4f}'.format(logit_k[0].item() - logit_k[1].item()))\n",
    "        # else:\n",
    "        #     save_result['caption_gap'].append('{:.4f}'.format(logit_k[0].item() - weighted.squeeze()[target].item()))\n",
    "\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if (i+1) % print_freq == 0:\n",
    "            progress.display(i)\n",
    "        \n",
    "    progress.display_summary()\n",
    "    return [each.avg for each in [top1, top2, top4, top8, top16, top32, top64, top128]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##parameters\n",
    "arch='ViT-B/16'\n",
    "n_ctx=4\n",
    "ctx_init=\"a_photo_of_a\"\n",
    "lr = 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the contect with given words: [a_photo_of_a]\n",
      "Initial context: \"a photo of a\"\n",
      "Number of context words (tokens): 4\n",
      "tuing parameters  {'prompt_learner.ctx'}\n",
      "=> Model created: visual backbone ViT-B/16\n",
      "=> Using native Torch AMP. Training in mixed precision.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "if test_sets in fewshot_datasets:\n",
    "    classnames = eval(\"{}_classes\".format(test_sets.lower()))\n",
    "model = get_coop(arch, test_sets, gpu, n_ctx, ctx_init)\n",
    "model_state = None\n",
    "\n",
    "cross_check = set()\n",
    "for name, param in model.named_parameters():\n",
    "\n",
    "    if \"prompt_learner\" not in name:\n",
    "        param.requires_grad_(False)\n",
    "    if param.requires_grad : cross_check.add(name)\n",
    "print(\"tuing parameters \", cross_check)\n",
    "\n",
    "print(\"=> Model created: visual backbone {}\".format(arch))\n",
    "\n",
    "assert gpu is not None\n",
    "torch.cuda.set_device(gpu)\n",
    "model = model.cuda(gpu)\n",
    "\n",
    "trainable_param = model.prompt_learner.parameters()\n",
    "optimizer = torch.optim.AdamW(trainable_param, lr)\n",
    "optim_state = deepcopy(optimizer.state_dict())\n",
    "\n",
    "# setup automatic mixed-precision (Amp) loss scaling\n",
    "scaler = torch.cuda.amp.GradScaler(init_scale=1000)\n",
    "\n",
    "print('=> Using native Torch AMP. Training in mixed precision.')\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating: Pets\n",
      "number of test samples: 3669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [09:09,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 399/3669]\tTime  1.423 ( 1.373)\tTop@1 100.00 ( 33.25)\tTop@2 100.00 ( 38.75)\tTop@4 100.00 ( 39.50)\tTop@8 100.00 ( 41.25)\tTop@16 100.00 ( 46.00)\tTop@32 100.00 ( 44.00)\tTop@64 100.00 ( 46.00)\tTop@128 100.00 ( 47.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [18:11,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 799/3669]\tTime  1.413 ( 1.365)\tTop@1 100.00 ( 30.88)\tTop@2 100.00 ( 36.38)\tTop@4 100.00 ( 36.75)\tTop@8 100.00 ( 38.88)\tTop@16 100.00 ( 40.50)\tTop@32 100.00 ( 39.88)\tTop@64   0.00 ( 42.38)\tTop@128 100.00 ( 42.88)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1200it [27:12,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [1199/3669]\tTime  1.295 ( 1.361)\tTop@1   0.00 ( 36.17)\tTop@2   0.00 ( 42.42)\tTop@4   0.00 ( 42.42)\tTop@8   0.00 ( 45.92)\tTop@16 100.00 ( 48.00)\tTop@32 100.00 ( 48.58)\tTop@64 100.00 ( 49.58)\tTop@128 100.00 ( 50.17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1600it [36:28,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [1599/3669]\tTime  1.387 ( 1.368)\tTop@1   0.00 ( 35.69)\tTop@2   0.00 ( 41.94)\tTop@4   0.00 ( 42.00)\tTop@8   0.00 ( 45.94)\tTop@16 100.00 ( 48.31)\tTop@32 100.00 ( 49.38)\tTop@64 100.00 ( 50.50)\tTop@128 100.00 ( 50.88)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [46:58,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [1999/3669]\tTime  1.960 ( 1.409)\tTop@1 100.00 ( 33.20)\tTop@2 100.00 ( 39.35)\tTop@4   0.00 ( 38.55)\tTop@8 100.00 ( 42.65)\tTop@16 100.00 ( 44.90)\tTop@32 100.00 ( 46.15)\tTop@64 100.00 ( 46.60)\tTop@128 100.00 ( 47.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2400it [57:43,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [2399/3669]\tTime  1.418 ( 1.443)\tTop@1 100.00 ( 33.71)\tTop@2 100.00 ( 40.62)\tTop@4 100.00 ( 40.96)\tTop@8 100.00 ( 44.83)\tTop@16 100.00 ( 46.75)\tTop@32 100.00 ( 48.25)\tTop@64   0.00 ( 48.08)\tTop@128   0.00 ( 48.29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2800it [1:08:00,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [2799/3669]\tTime  1.395 ( 1.457)\tTop@1   0.00 ( 36.36)\tTop@2   0.00 ( 43.32)\tTop@4   0.00 ( 43.68)\tTop@8 100.00 ( 47.29)\tTop@16 100.00 ( 49.11)\tTop@32 100.00 ( 50.61)\tTop@64 100.00 ( 50.29)\tTop@128 100.00 ( 50.11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3200it [1:18:24,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [3199/3669]\tTime  1.285 ( 1.470)\tTop@1   0.00 ( 39.09)\tTop@2   0.00 ( 45.50)\tTop@4   0.00 ( 46.34)\tTop@8   0.00 ( 49.44)\tTop@16 100.00 ( 51.66)\tTop@32   0.00 ( 53.22)\tTop@64   0.00 ( 52.75)\tTop@128   0.00 ( 52.16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3600it [1:28:15,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [3599/3669]\tTime  1.373 ( 1.471)\tTop@1 100.00 ( 38.25)\tTop@2 100.00 ( 44.78)\tTop@4 100.00 ( 46.14)\tTop@8 100.00 ( 49.33)\tTop@16 100.00 ( 51.92)\tTop@32 100.00 ( 53.67)\tTop@64 100.00 ( 53.78)\tTop@128 100.00 ( 53.11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3669it [1:30:15,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *  Top@1 38.675 Top@2 45.244 Top@4 46.716 Top@8 50.041 Top@16 52.603 Top@32 54.402 Top@64 54.538 Top@128 53.938\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'caption_correct'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'caption_correct'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 55\u001b[0m\n\u001b[1;32m     51\u001b[0m df\u001b[39m.\u001b[39mto_csv(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, \u001b[39m'\u001b[39m\u001b[39mcaption_ensemble_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(set_id)))\n\u001b[1;32m     53\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path,\u001b[39m'\u001b[39m\u001b[39mcaption_ensemble_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(set_id)), \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 55\u001b[0m     cap_corr_ind \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mloc[df[\u001b[39m'\u001b[39;49m\u001b[39mcaption_correct\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto_list()\n\u001b[1;32m     56\u001b[0m     \u001b[39m#image accuracy, caption ensemble accuracy\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     f\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39m1. Caption Accuracy \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat( \u001b[39mlen\u001b[39m(cap_corr_ind)\u001b[39m/\u001b[39mtotal_length))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'caption_correct'"
     ]
    }
   ],
   "source": [
    "\n",
    "resolution = 224\n",
    "workers = 4\n",
    "dataset_mode = 'test'\n",
    "data = '/data/seongha'\n",
    "\n",
    "from collections import defaultdict\n",
    "    # norm stats from clip.load()\n",
    "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                     std=[0.26862954, 0.26130258, 0.27577711])\n",
    "    \n",
    "    # iterating through eval datasets\n",
    "datasets = test_sets.split(\"/\")\n",
    "results = {}\n",
    "\n",
    "for set_id in datasets:\n",
    "    Dict = defaultdict(list)\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize(resolution, interpolation=BICUBIC),\n",
    "        transforms.CenterCrop(resolution),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    \n",
    "    batchsize = 1\n",
    "    print(\"evaluating: {}\".format(set_id))\n",
    "    classnames = eval(\"{}_classes\".format(set_id.lower()))\n",
    "    model.reset_classnames(classnames, arch)\n",
    "\n",
    "    val_dataset = build_dataset(set_id, data_transform, data, mode=dataset_mode)\n",
    "    total_length = len(val_dataset)\n",
    "    print(\"number of test samples: {}\".format(len(val_dataset)))\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=batchsize, shuffle=False,\n",
    "                num_workers=workers, pin_memory=True)\n",
    "        \n",
    "    results[set_id] = test_time_adapt_eval(val_loader, model, model_state, optimizer, optim_state, scaler, Dict)\n",
    "    del val_dataset, val_loader\n",
    "\n",
    "    # try:\n",
    "    #     print(\"=> Acc. on testset [{}]: @1 {}/ @2 {}/ @3 {}/ @4 {}/ @5 {}\".format(set_id, results[set_id][0], results[set_id][1], results[set_id][2], results[set_id][3], results[set_id[4], results[set_id][5]]))\n",
    "    # except:\n",
    "    #     print(\"=> Acc. on testset [{}]: {}\".format(set_id, results[set_id]))\n",
    "\n",
    "    df = pd.DataFrame(Dict)\n",
    "    df = df.reset_index()\n",
    "\n",
    "    path = './notebook/caption_ensemble'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    df.to_csv(os.path.join(path, 'caption_ensemble_{}.csv'.format(set_id)))\n",
    "    \n",
    "    with open(os.path.join(path,'caption_ensemble_{}.txt'.format(set_id)), 'w') as f:\n",
    "        \n",
    "        cap_corr_ind = df.loc[df['caption_correct'] == 1, 'index'].to_list()\n",
    "        #image accuracy, caption ensemble accuracy\n",
    "        f.write(\"1. Caption Accuracy {:.4f}\".format( len(cap_corr_ind)/total_length))\n",
    "        #entropy, logit gap\n",
    "        f.write(\"Entropy & Logit Gap\\n\")\n",
    "        cap_correct = df.loc[df['caption_correct'] == 1]\n",
    "        f.write(\"correct\\n\")\n",
    "        f.write(\" {}\\n\".format(str(cap_correct.shape)))\n",
    "        f.write(\"top1 - top2 mean, std\\n\")\n",
    "        f.write(\"{:.4f} {:.4f}\\n\".format(cap_correct['caption_gap'].astype(float).mean(), cap_correct['caption_gap'].astype(float).std() ))\n",
    "        f.write(\"Entropy mean {}\\n\".format(str(cap_correct['caption_entropy'].astype(float).mean())))\n",
    "        f.write(\"\")\n",
    "        cap_wrong = df.loc[df['caption_correct'] == 0]\n",
    "        f.write(\"wrong\\n\")\n",
    "        f.write(\" {}\\n\".format(str(cap_wrong.shape)))\n",
    "        f.write(\"pred(top1) - target mean, std\\n\")\n",
    "        f.write(\"{:.4f} {:.4f}\\n\".format(cap_wrong['caption_gap'].astype(float).mean(), cap_wrong['caption_gap'].astype(float).std() ))\n",
    "        f.write(\"Entropy mean {}\\n\".format(str(cap_wrong['caption_entropy'].astype(float).mean())))\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
