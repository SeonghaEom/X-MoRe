{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "\n",
    "import time\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "import torchvision.models as models\n",
    "\n",
    "import clip\n",
    "from clip.custom_clip import get_coop\n",
    "from clip.cocoop import get_cocoop\n",
    "from data.imagnet_prompts import imagenet_classes\n",
    "from data.datautils import AugMixAugmenter, build_dataset\n",
    "from utils.tools import Summary, AverageMeter, ProgressMeter, load_model_weight, set_random_seed, create_logger\n",
    "from data.cls_to_names import *\n",
    "from data.fewshot_datasets import fewshot_datasets\n",
    "from data.imagenet_variants import thousand_k_to_200, imagenet_a_mask, imagenet_r_mask, imagenet_v_mask\n",
    "from clip_retrieval.clip_client import ClipClient, Modality\n",
    "\n",
    "from collections import defaultdict\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "client = ClipClient(\n",
    "    url=\"http://127.0.0.1:1234/knn-service\",\n",
    "    indice_name='laion_400m',\n",
    "    modality=Modality.IMAGE,\n",
    "    num_images=20,\n",
    "    deduplicate=False,\n",
    ")\n",
    "client_backup = ClipClient(\n",
    "    url=\"http://127.0.0.1:1234/knn-service\",\n",
    "    indice_name='laion_400m',\n",
    "    modality=Modality.IMAGE,\n",
    "    num_images=200,\n",
    "    deduplicate=False,\n",
    ")\n",
    "\n",
    "client_backup2 = ClipClient(\n",
    "    url=\"http://127.0.0.1:1234/knn-service\",\n",
    "    indice_name='laion_400m',\n",
    "    modality=Modality.IMAGE,\n",
    "    num_images=500,\n",
    "    deduplicate=False,\n",
    ")\n",
    "\n",
    "## Class to names mapping\n",
    "fewshot_datasets = ['DTD', 'Flower102', 'Food101', 'Cars', 'SUN397', \n",
    "                    'Aircraft', 'Pets', 'Caltech101', 'UCF101', 'eurosat']\n",
    "test_sets = 'DTD/Flower102/Food101/Cars/SUN397/Aircraft/Pets/Caltech101/UCF101/eurosat'#Parameters\n",
    "tta_steps = 1\n",
    "which_loss = \"cosine\"\n",
    "gpu = 7\n",
    "print_freq = 100\n",
    "arch='ViT-B/16'\n",
    "n_ctx=4\n",
    "ctx_init=\"a_photo_of_a\"\n",
    "lr = 5e-3\n",
    "global retrieve_K\n",
    "retrieve_K= 1\n",
    "\n",
    "tau_dict = {'DTD': {'i_tau': 0.0, 'c_tau': 0.08},\n",
    " 'Flower102': {'i_tau': 0.46, 'c_tau': 0.0},\n",
    " 'Food101': {'i_tau': 0.0, 'c_tau': 0.99},\n",
    " 'Cars': {'i_tau': 0.0, 'c_tau': 0.99},\n",
    " 'SUN397': {'i_tau': 0.0, 'c_tau': 0.72},\n",
    " 'Aircraft': {'i_tau': 0.0, 'c_tau': 0.37},\n",
    " 'Pets': {'i_tau': 1.47, 'c_tau': 0.0},\n",
    " 'Caltech101': {'i_tau': 0.21, 'c_tau': 0.0},\n",
    " 'UCF101': {'i_tau': 0.0, 'c_tau': 0.68},\n",
    " 'eurosat': {'i_tau': 0.0, 'c_tau': 0.54}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def avg_entropy(outputs):\n",
    "    logits = outputs - outputs.logsumexp(dim=-1, keepdim=True) # logits = outputs.log_softmax(dim=1) [N, 1000]\n",
    "    # logits = outputs.log_softmax(dim=-1) #[N, 1000]\n",
    "    avg_logits = logits.logsumexp(dim=0) - np.log(logits.shape[0]) # avg_logits = logits.mean(0) [1, 1000]\n",
    "    # avg_logits = logits.mean(0) #[1, 1000]\n",
    "    # print(avg_logits\n",
    "    \n",
    "    min_real = torch.finfo(avg_logits.dtype).min\n",
    "    avg_logits = torch.clamp(avg_logits, min=min_real)\n",
    "    return -((avg_logits) * (torch.exp(avg_logits))).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "def select_confident_samples(logits, top):\n",
    "    batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "    idx = torch.argsort(batch_entropy, descending=False)[:int(batch_entropy.size()[0] * top)]\n",
    "    return logits[idx], idx\n",
    "\n",
    "def Cosine(img_logit, cap_logit, model):\n",
    "    # (retrieveK, C)\n",
    "    cosine = torch.nn.CosineEmbeddingLoss(reduce=True)\n",
    "    y_hat = F.softmax(torch.sigmoid(model.alpha)* img_logit + (1-torch.sigmoid(model.alpha)) * cap_logit, dim=-1)\n",
    "    # print(y_hat.shape, img_logit.shape)\n",
    "    return cosine(y_hat, y_hat, torch.ones(img_logit.shape[0]).cuda(gpu))\n",
    "    \n",
    "def JSdiv(logit, logit2, model):\n",
    "    # assert logit.shape == logit2.shape, (logit.shape, logit2.shape)\n",
    "    class_num = logit.size()[-1]\n",
    "    \n",
    "    kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    prob1 = F.softmax(logit, dim=-1)\n",
    "    prob2 = F.softmax(logit2, dim=-1)\n",
    "    total_m = 0.5 *prob1 + 0.5*prob2\n",
    "    loss = 0.0\n",
    "    loss += torch.sigmoid(model.alpha)* kl_loss(F.log_softmax(logit, dim=-1), total_m)\n",
    "    loss += (1-torch.sigmoid(model.alpha)) * kl_loss(F.log_softmax(logit2, dim=-1), total_m)\n",
    "    return loss\n",
    "def JeffreyDiv(logit, logit2, model):\n",
    "    class_num = logit.size()[-1]\n",
    "    uni_ent = avg_entropy(torch.ones(class_num))\n",
    "    uni_ent.requires_grad_(True)\n",
    "    # print(uni_ent.requires_grad)\n",
    "    a = torch.sigmoid(model.c_tau)\n",
    "    alpha = (1 - ((avg_entropy(logit2) * a)/uni_ent))\n",
    "    input = F.log_softmax(logit, dim=-1)\n",
    "    input.requires_grad_(True)\n",
    "    target = F.softmax(logit2, dim=-1)\n",
    "    #(1-entropy(p)/entropy(Unif(C)))\n",
    "    input_ = F.log_softmax(logit2, dim=-1)\n",
    "    target_ = F.softmax(logit, dim=-1)\n",
    "    b = torch.sigmoid(model.i_tau)\n",
    "    beta = (1 - ((avg_entropy(logit) * b)/uni_ent))\n",
    "    # total = alpha+beta #weight sum 1\n",
    "    one = alpha * kl_loss(input, target.detach()) #image를 caption에 맞춤\n",
    "    two = beta * kl_loss(input_, target_.detach())#caption을 Image에 맞춤\n",
    "    # assert alpha.requires_grad and beta.requires_grad and input.requires_grad and input_.requires_grad and logit.requires_grad and alpha.requires_grad\n",
    "    # print(alpha.requires_grad, beta.requires_grad, input.requires_grad, input_.requires_grad, logit.requires_grad, alpha.requires_grad)\n",
    "    return one + two\n",
    "\n",
    "def KLreliable(logit, logit2):\n",
    "    assert logit.shape == logit2.shape, (logit.shape, logit2.shape)\n",
    "    kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    if average_entropy(logit) < average_entropy(logit2):\n",
    "        # first is more reliable\n",
    "        # print(\"image is more reliable\")\n",
    "        input = F.log_softmax(logit2, dim=-1)\n",
    "        target = F.softmax(logit, dim=-1)\n",
    "        return kl_loss(input, target)\n",
    "    else:\n",
    "        # print(\"caption is more reliable\")\n",
    "        input = F.log_softmax(logit, dim=-1)\n",
    "        target = F.softmax(logit2, dim=-1)\n",
    "        return kl_loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy(output, target, topk=(1,), caption=None, logger=None):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        pred = output.argmax(dim=-1)\n",
    "        assert pred.shape == target.shape, (pred.shape, target.shape)\n",
    "        correct = pred.eq(target).sum()\n",
    "        return correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def return_caption(img_path, retrieve_K=1):\n",
    "    try:\n",
    "        query_res = client.query(image=img_path)\n",
    "        assert len(query_res) >= retrieve_K\n",
    "        query_res = query_res[:retrieve_K]\n",
    "        retrieved_txt= [D['caption'] for D in query_res]\n",
    "        retrieved_url = [D['url'] for D in query_res]\n",
    "        retrieved_score = [D['similarity'] for D in query_res]\n",
    "        return retrieved_txt, retrieved_score\n",
    "    except:\n",
    "        query_res = client_backup2.query(image=img_path)\n",
    "        if len(query_res) >= retrieve_K:\n",
    "            query_res = query_res[:retrieve_K]\n",
    "            retrieved_txt= [D['caption'] for D in query_res]\n",
    "            retrieved_url = [D['url'] for D in query_res]\n",
    "            retrieved_score = [D['similarity'] for D in query_res]\n",
    "            return retrieved_txt, retrieved_score\n",
    "        else:\n",
    "            return None, None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2986) tensor(0.2986) tensor(0.2986)\n",
      "tensor(0.2986)\n"
     ]
    }
   ],
   "source": [
    "test1= torch.tensor([2.0, 2, 2, 2, 3, 4])\n",
    "def confidence(x):\n",
    "    uniform_ent = avg_entropy(torch.ones(x.shape[-1]))\n",
    "    logit_ent = avg_entropy(x)\n",
    "    logit_ent1 = avg_entropy(x/x.norm(dim=-1))\n",
    "    print(logit_ent, logit_ent1, uniform_ent)\n",
    "    return logit_ent\n",
    "\n",
    "print(confidence(test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_time_tuning(model, inputs, optimizer, scaler, imagepath = None):\n",
    "    # Entropy + Triplet loss function * 0.5\n",
    "    # Triplet loss function, anchor = retrieved vocab, positive = top5, negative = bottom5\n",
    "    for j in range(tta_steps):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output_img, text_features = model(inputs) # bs, n_cls, (1, 1000), logit/ n_cls, 512\n",
    "            # logit_scale = model.logit_scale.exp()\n",
    "            # ent = avg_entropy(output_img)\n",
    "            #caption\n",
    "            retrieved_Caption, retrieved_score = return_caption(imagepath, retrieve_K=retrieve_K)\n",
    "            \n",
    "            if retrieved_Caption==None:\n",
    "                return None\n",
    "            # print(retrieved_Caption)\n",
    "            output_caption = model.caption_ensemble(retrieved_Caption)\n",
    "            \n",
    "            # print(output_img.norm(dim=-1), output_caption.norm(dim=-1))\n",
    "            # weighted = []\n",
    "            # for score, logit in zip(retrieved_score, output_caption):\n",
    "            #     logit = torch.nn.functional.softmax(logit, dim=-1)\n",
    "            #     weighted.append(score/sum(retrieved_score) * logit)\n",
    "\n",
    "            # tmp = torch.sum(torch.cat(weighted[:retrieve_K]).reshape(retrieve_K, -1), axis=0).reshape(1, -1)\n",
    "            # loss = []\n",
    "            # loss_val = JeffreyDiv(output_img/output_img.norm(dim=-1, keepdim=True), output_caption/output_caption.norm(dim=-1, keepdim=True), model)\n",
    "            loss_val = JeffreyDiv(output_img, output_caption, model)\n",
    "            # print(loss_val)\n",
    "            # loss_val += Cosine(output_img, output_caption, model)\n",
    "\n",
    "            optimizer.zero_grad() \n",
    "            scaler.scale(loss_val).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "    # print(\"empty caption {}\".format(cnt_empty))\n",
    "    return retrieved_Caption, output_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_time_adapt_eval(val_loader, model, model_state, optimizer, optim_state, scaler, save_result=False, set_id=''):\n",
    "    batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f', Summary.AVERAGE)\n",
    "    top2 = AverageMeter('Acc@2', ':6.2f', Summary.AVERAGE)\n",
    "    top3 = AverageMeter('Acc@3', ':6.2f', Summary.AVERAGE)\n",
    "    top4 = AverageMeter('Acc@4', ':6.2f', Summary.AVERAGE)\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f', Summary.AVERAGE)\n",
    "    \n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, top1, top2, top3, top4, top5],\n",
    "        prefix='Test: ',\n",
    "        logger = None)\n",
    "\n",
    "    # reset model and switch to evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.reset()\n",
    "    end = time.time()\n",
    "    cnt_empty = 0\n",
    "    assert save_result != None\n",
    "    ## to measure accuracy\n",
    "    total_images = 0\n",
    "    correct_images = 0\n",
    "    \n",
    "    ## image average entropy mean\n",
    "    mean_stat = torch.tensor([], dtype=torch.float32)\n",
    "    # cmean_stat = dict()\n",
    "    \n",
    "    ## caption average entropy mean\n",
    "    cap_mean_stat = torch.tensor([], dtype=torch.float32)\n",
    "    # cap_cmean_stat = 0\n",
    "    \n",
    "    # count accuracy of when using caption!\n",
    "    cnt_cap = 0\n",
    "    cnt_cap_correct = 0\n",
    "    for i, (images, target, imagepath) in tqdm(enumerate(val_loader)): \n",
    "        # print(len(image))\n",
    "        assert gpu is not None\n",
    "        # print(\"Image Path \", imagepath[0])\n",
    "        if isinstance(images, list):\n",
    "            for k in range(len(images)):\n",
    "                images[k] = images[k].cuda(gpu, non_blocking=True)\n",
    "            image = images[0]\n",
    "            images = torch.cat(images, dim=0)\n",
    "        else:\n",
    "            image = images.cuda(gpu, non_blocking=True)\n",
    "        target = target.cuda(gpu, non_blocking=True)\n",
    "        ### One time training\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.reset()\n",
    "        optimizer.load_state_dict(optim_state)\n",
    "        \n",
    "        assert not torch.isinf(model.i_tau).any(), model.i_tau\n",
    "        assert not torch.isnan(model.c_tau).any(), model.c_tau\n",
    "        # TTA\n",
    "        retrieved_Caption, caption_logit = test_time_tuning(model, image, optimizer, scaler, imagepath[0])\n",
    "        # if retrieved_Caption==None:\n",
    "            # cnt_empty +=1\n",
    "        # print(model.gamma_cap, model.gamma_img)\n",
    "        assert model.i_tau.is_leaf == True\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                #image\n",
    "                output_img = model.inference(image.cuda(gpu, non_blocking=True))\n",
    "                #caption\n",
    "                # retrieved_Caption, retrieved_score = return_caption(imagepath[0], retrieve_K=retrieve_K)\n",
    "                # if retrieved_Caption == None:\n",
    "                #     cnt_empty +=1\n",
    "                    # continue\n",
    "                # caption_logit = model.caption_ensemble(retrieved_Caption)\n",
    "        #TODO: scaled normalization for each modality\n",
    "        conf_img = avg_entropy(output_img)\n",
    "        conf_cap = avg_entropy(caption_logit)\n",
    "        # print(conf_img.item(), conf_cap.item())\n",
    "        if conf_img > torch.mean(mean_stat) * (1+model.i_tau) and conf_cap < torch.mean(cap_mean_stat) * (1-model.c_tau):\n",
    "            # 이미자 못 맞추는걸 캡션으로 해보자\n",
    "            # print(caption_logit.shape)\n",
    "            correct_ = accuracy(caption_logit, target, topk=(1, 2, 3, 4, 5), caption=None, logger=None).item()\n",
    "            cnt_cap += 1\n",
    "            if correct_ : cnt_cap_correct +=1\n",
    "        else:\n",
    "            correct_ = accuracy(output_img , target, topk=(1, 2, 3, 4, 5), caption=None, logger=None).item()\n",
    "        #update mean\n",
    "        mean_stat = torch.cat([mean_stat, torch.tensor([conf_img],dtype=torch.float32)])\n",
    "        cap_mean_stat = torch.cat([cap_mean_stat, torch.tensor([conf_cap], dtype=torch.float32)])\n",
    "        total_images += 1\n",
    "        correct_images += correct_\n",
    "\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if (i+1) % print_freq == 0:\n",
    "            # progress.display(i)\n",
    "            \n",
    "            print(\"accuracy  \", correct_images/total_images)\n",
    "            print(\"image confidence score mean stat\", torch.mean(mean_stat).item())\n",
    "            print(\"caption confidence score mean stat\", torch.mean(cap_mean_stat).item())\n",
    "            print(\"count caption correct {} out of {}\".format(cnt_cap_correct, cnt_cap))\n",
    "            print(\"c tau {} i tau {}\".format(model.c_tau, model.i_tau))\n",
    "            save_result['accuracy'].append(correct_images/total_images)\n",
    "            save_result['image_conf_mean'].append(torch.mean(mean_stat).item())\n",
    "            save_result['caption_conf_mean'].append(torch.mean(cap_mean_stat).item())\n",
    "            save_result['cap_cnt'].append(cnt_cap)\n",
    "            save_result['cap_corr'].append(cnt_cap_correct)\n",
    "            # save_result['c_tau'].append(c_tau)\n",
    "            # save_result['i_tau'].append(i_tau)\n",
    "            df = pd.DataFrame(save_result)\n",
    "            df = df.reset_index()\n",
    "            path = './notebook/JSdiv/{}'.format(arch.replace('/', ''))\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            df.to_csv(os.path.join(path, 'JSdiv_{}.csv'.format(set_id)))\n",
    "    \n",
    "    print(\"empty caption count = {}\".format(cnt_empty))\n",
    "    # progress.display_summary()\n",
    "    print(\"Accuracy: {}\".format(correct_images/total_images) )\n",
    "    return save_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype  torch.float32\n",
      "Initializing the contect with given words: [a_photo_of_a]\n",
      "Initial context: \"a photo of a\"\n",
      "Number of context words (tokens): 4\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "if test_sets in fewshot_datasets:\n",
    "    classnames = eval(\"{}_classes\".format(test_sets.lower()))\n",
    "model = get_coop(arch, test_sets, gpu, n_ctx, ctx_init)\n",
    "\n",
    "model_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuning parameters  {'i_tau', 'c_tau'}\n",
      "{'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'amsgrad': False, 'foreach': None, 'maximize': False, 'capturable': False, 'params': [0]}, {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'amsgrad': False, 'foreach': None, 'maximize': False, 'capturable': False, 'params': [1]}]}\n",
      "=> Using native Torch AMP. Training in mixed precision.\n",
      "retrieve K: 1\n",
      "evaluating: DTD\n",
      "tuing parameters  {'i_tau', 'c_tau'}\n",
      "number of test samples: 1692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:39,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy   0.36\n",
      "image confidence score mean stat 1.7302119731903076\n",
      "caption confidence score mean stat 1.2301819324493408\n",
      "count caption correct 34 out of 97\n",
      "c tau Parameter containing:\n",
      "tensor(-2.4413, device='cuda:7', requires_grad=True) i tau Parameter containing:\n",
      "tensor(-16.1169, device='cuda:7', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [01:17,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy   0.325\n",
      "image confidence score mean stat 1.7156692743301392\n",
      "caption confidence score mean stat 1.1788921356201172\n",
      "count caption correct 63 out of 197\n",
      "c tau Parameter containing:\n",
      "tensor(-2.4413, device='cuda:7', requires_grad=True) i tau Parameter containing:\n",
      "tensor(-16.1171, device='cuda:7', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [01:56,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy   0.29\n",
      "image confidence score mean stat 1.6457427740097046\n",
      "caption confidence score mean stat 1.2010986804962158\n",
      "count caption correct 85 out of 297\n",
      "c tau Parameter containing:\n",
      "tensor(-2.4413, device='cuda:7', requires_grad=True) i tau Parameter containing:\n",
      "tensor(-16.1169, device='cuda:7', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [02:35,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy   0.2675\n",
      "image confidence score mean stat 1.6830614805221558\n",
      "caption confidence score mean stat 1.2281428575515747\n",
      "count caption correct 105 out of 397\n",
      "c tau Parameter containing:\n",
      "tensor(-2.4413, device='cuda:7', requires_grad=True) i tau Parameter containing:\n",
      "tensor(-16.1169, device='cuda:7', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [03:14,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy   0.27\n",
      "image confidence score mean stat 1.696144938468933\n",
      "caption confidence score mean stat 1.2303662300109863\n",
      "count caption correct 133 out of 497\n",
      "c tau Parameter containing:\n",
      "tensor(-2.4413, device='cuda:7', requires_grad=True) i tau Parameter containing:\n",
      "tensor(-16.1169, device='cuda:7', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [03:53,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy   0.26166666666666666\n",
      "image confidence score mean stat 1.6995246410369873\n",
      "caption confidence score mean stat 1.2460707426071167\n",
      "count caption correct 155 out of 597\n",
      "c tau Parameter containing:\n",
      "tensor(-2.4413, device='cuda:7', requires_grad=True) i tau Parameter containing:\n",
      "tensor(-16.1169, device='cuda:7', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "660it [04:16,  2.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "662it [04:17,  2.56it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "resolution = 224\n",
    "workers = 4\n",
    "dataset_mode = 'test'\n",
    "data = '/data/seongha'\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "    # norm stats from clip.load()\n",
    "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                     std=[0.26862954, 0.26130258, 0.27577711])\n",
    "    \n",
    "    # iterating through eval datasets\n",
    "datasets = test_sets.split(\"/\")\n",
    "results = {}\n",
    "\n",
    "# with open('inferece_image_caption{}.txt'.format(test_sets), 'w') as f:\n",
    "    # sys.stdout = f\n",
    "for set_id in datasets:\n",
    "\n",
    "    i_tau = tau_dict[set_id]['i_tau']\n",
    "    c_tau = tau_dict[set_id]['c_tau']\n",
    "\n",
    "    model.set_tau(i_tau=i_tau, c_tau = c_tau)\n",
    "    assert gpu is not None\n",
    "    torch.cuda.set_device(gpu)\n",
    "    model = model.cuda(gpu)\n",
    "\n",
    "    trainable_param = [\n",
    "        # {'params' : model.prompt_learner.parameters()},\n",
    "                        {'params' : model.c_tau, 'lr': 1e-3},\n",
    "                        {'params' : model.i_tau, 'lr': 1e-3},\n",
    "                        ]\n",
    "                    \n",
    "    # trainable_param = model.parameters()\n",
    "    optimizer = torch.optim.AdamW(trainable_param, lr)\n",
    "    optim_state = deepcopy(optimizer.state_dict())\n",
    "\n",
    "    cross_check = set()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name not in \"i_tau\" and name not in 'c_tau': param.requires_grad = False\n",
    "        if param.requires_grad : cross_check.add(name)\n",
    "    print(\"tuning parameters \", cross_check)\n",
    "    print(optim_state)\n",
    "    # setup automatic mixed-precision (Amp) loss scaling\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    # scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    print('=> Using native Torch AMP. Training in mixed precision.')\n",
    "    for retrieve_K in [1]:\n",
    "        print(\"retrieve K: {}\".format(retrieve_K))\n",
    "        Dict = defaultdict(list)\n",
    "        data_transform = transforms.Compose([\n",
    "            transforms.Resize(resolution, interpolation=BICUBIC),\n",
    "            transforms.CenterCrop(resolution),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "        # base_transform = transforms.Compose([\n",
    "        #     transforms.Resize(224, interpolation=BICUBIC),\n",
    "        #     transforms.CenterCrop(224)])\n",
    "        # preprocess = transforms.Compose([\n",
    "        #     transforms.ToTensor(),\n",
    "        #     normalize])\n",
    "        # data_transform = AugMixAugmenter(base_transform, preprocess, n_views=retrieve_K-1, \n",
    "        #                                     augmix=len(set_id)>1)\n",
    "        batchsize = 1\n",
    "        print(\"evaluating: {}\".format(set_id))\n",
    "        classnames = eval(\"{}_classes\".format(set_id.lower()))\n",
    "        model.reset_classnames(classnames, arch)\n",
    "        with torch.no_grad():\n",
    "            model.reset()\n",
    "\n",
    "            # model.alpha = torch.nn.Parameter(torch.tensor(0.0))\n",
    "            # model.i_tau = torch.nn.Parameter(torch.tensor(tau_dict[set_id]['i_tau']))\n",
    "            # model.c_tau = torch.nn.Parameter(torch.tensor(tau_dict[set_id]['c_tau']))\n",
    "        optimizer.load_state_dict(optim_state)\n",
    "        val_dataset = build_dataset(set_id, data_transform, data, mode=dataset_mode)\n",
    "        total_length = len(val_dataset)\n",
    "\n",
    "        assert next(model.parameters()).is_cuda and  model.i_tau.is_cuda, model.i_tau.is_cuda\n",
    "        \n",
    "        cross_check = set()\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad : cross_check.add(name)\n",
    "        print(\"tuing parameters \", cross_check)\n",
    "        print(\"number of test samples: {}\".format(len(val_dataset)))\n",
    "\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "                    val_dataset,\n",
    "                    batch_size=batchsize, shuffle=True,\n",
    "                    num_workers=workers, pin_memory=True)\n",
    "            \n",
    "        results = test_time_adapt_eval(val_loader, model, model_state, optimizer, optim_state, scaler, Dict, set_id)\n",
    "            # assert len(Dict['image_path']) == len(Dict['caption']) and len(Dict['image_correct']) == len(Dict['caption_correct']), [len(v) for k, v in Dict.items()]\n",
    "        df = pd.DataFrame(results)\n",
    "        df = df.reset_index()\n",
    "\n",
    "        path = './notebook/JSdiv'\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        df.to_csv(os.path.join(path, 'JSdiv_{}.csv'.format(set_id)))\n",
    "del val_dataset, val_loader\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
